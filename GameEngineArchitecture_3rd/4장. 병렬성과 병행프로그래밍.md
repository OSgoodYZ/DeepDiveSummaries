# 4장 병렬성과 병행 프로그래밍

## 4.1 병행성과 병렬성에 대한 정의

### 4.1.1 병행성 (Concurrency)

- 하나의 문제를 해결하기 위해 여러 개의 제어 흐름을 활용.
- 제어 흐름:
  - 한 프로세스 내 여러 스레드(thread) 사용
  - 여러 컴퓨터에 분산된 프로세스
  - 파이버(fiber)나 코루틴(coroutine) 사용
- 병행 프로그래밍과 순차적 프로그래밍의 핵심 차이: **공유 데이터에 대한 접근**
- **독립적 데이터**를 처리하는 것은 병행성이 아님.

### 병행성의 문제
- 공유 데이터에 대한 일관성, 정확성 확보가 중요
- **데이터 경쟁 상태(race condition)** 발생 위험
- 병행성의 핵심은 데이터 경쟁을 **찾고 제거**하는 것

---

### 4.1.2 병렬성 (Parallelism)

- **둘 이상의 하드웨어가 동시에 작동**하는 상태
- 예: 하나의 병렬 컴퓨터 → 여러 작업 동시 수행
- 반대: 직렬(serial) 컴퓨터 → 한 번에 하나의 작업만

#### 과거:
- 1989년 이전 대부분의 소비자용 컴퓨터는 직렬 구조
  - Apple II, 6502 CPU, Intel 8086/80286/80386 등

#### 현재:
- 멀티코어 CPU (Intel Core i7, AMD Ryzen 등)
- 다양한 병렬 컴퓨팅 형태로 확장 가능
  - 예: CPU 내부 ALU 병렬 구성, 컴퓨터 클러스터(cluster) 등

---

#### 4.1.2.1 묵시적 병렬성과 명시적 병렬성

* 묵시적 병렬성 (Implicit Parallelism)

  - 하나의 명령어 스트림을 효율적으로 처리하기 위해 CPU 내부 구성 요소 사용
  - 명령어 수준 병렬성(ILP: Instruction Level Parallelism)
  - 예시:
    - 파이프라인(pipeline)
    - 슈퍼스칼라 아키텍처(superscalar architecture)
    - VLIW 아키텍처

* 명시적 병렬성 (Explicit Parallelism)
  - 둘 이상의 명령어 스트림을 처리
  - CPU/시스템에 중복된 하드웨어 사용
  - **병행 소프트웨어**를 효율적으로 처리하기 위한 하드웨어 설계

    - 예시:
      - 하이퍼스레드 CPU
      - 멀티코어 CPU
      - 멀티프로세서 컴퓨터
      - 컴퓨터 클러스터(cluster)
      - 그리드 컴퓨팅(grid computing)
      - 클라우드 컴퓨팅(cloud computing)

---

### 4.1.3 작업 병렬성과 데이터 병렬성

- 병렬성을 이해하는 또 다른 방법은 **작업의 종류**에 따라 나누는 것.
  
#### ● 작업 병렬성 (Task Parallelism)
- 이질적인 명령어들이 병렬적으로 여러 실행됨 → 작업 병렬성
- 예: 한 코어에서 애니메이션 계산, 다른 코어에서는 충돌 체크 수행

#### ● 데이터 병렬성 (Data Parallelism)
- 하나의 동일한 명령어가 여러 데이터를 병렬적으로 수행하는 것
- 예: 4개의 코어가 각각 250개의 스키닝 행렬 계산

> 대부분의 병렬 프로그램은 이 두 가지 병렬성을 혼합하여 사용

---

### 4.1.4 플린 분류 (Flynn's Taxonomy)

- 마이클 J. 플린이 제안한 병렬성의 분류 방식 (1996)
- 병렬성을 제어 흐름(명령어 스트림)과 데이터 스트림 수의 조합으로 4가지로 구분

#### ● SISD (Single Instruction, Single Data)
- 하나의 명령어 스트림이 하나의 데이터 스트림 처리
- 가장 기본적인 구조

#### ● MIMD (Multiple Instruction, Multiple Data)
- 여러 명령어 스트림이 여러 데이터 스트림 처리
- 대표적인 병렬 구조

#### ● SIMD (Single Instruction, Multiple Data)
- 하나의 명령어 스트림이 여러 데이터 스트림을 동시에 처리

#### ● MISD (Multiple Instruction, Single Data)
- 여러 명령어 스트림이 하나의 데이터 스트림을 처리
- 거의 사용되지 않으며, 예외적인 상황에만 사용됨 (예: 오류 복구, 핫 스페어 등)

---

#### 4.1.4.1 단일 데이터와 다중 데이터

- "데이터 스트림"은 단순한 숫자 배열이 아님 → 연산자 중심의 설명 필요
- 대부분 연산자는 2개의 입력을 받아 1개의 결과 출력

#### ● SISD
- 하나의 ALU가 곱 연산 후 나누기 연산 수행  
  `mul a, b` → `div c, d`

#### ● MIMD
- 2개의 ALU가 서로 독립적인 명령어 스트림을 병렬 수행  
  `ALU0: mul a, b`, `ALU1: sub g, h`

#### ● 시간 분할 MIMD
- 하나의 ALU가 시분할 방식으로 2개의 명령어 스트림을 처리

#### ● SIMD
- 벡터 처리 유닛(VPU)을 사용해 한 쌍의 4원소 벡터를 입력받아 연산

#### ● MISD
- 동일한 명령어 스트림을 두 ALU가 나눠 수행하여 **이론적으로 동일한 결과**를 출력
- 핫 스페어(hot spare)로 사용되기도 함

---

#### 4.1.4.2 GPU 병렬성: SIMT

- SIMT (Single Instruction Multiple Thread): GPU 설계에서 SIMD와 MIMD의 혼합 구조
- **멀티스레딩** 기법을 사용해 명령어 스트림을 시간 분할하여 병렬 처리
- 다양한 제조사들이 채택하는 디자인
- ‘매니코어(manycore)’라는 표현도 사용됨

---

### 4.1.5 병행성과 병렬성의 직교적 성질

- 병렬 하드웨어 없이도 병행 소프트웨어는 가능
- 병행성과 병렬성은 **서로 독립적 개념**이지만, 함께 쓰이면 성능 향상 가능

#### 예:
- 단일 스레드 CPU에서도 병행 소프트웨어 실행 가능 (멀티태스킹 등)
- 단일 명령어 수준 병렬성도 성능 향상 목적

---

### 4.1.6 4장의 로드맵

- 4.2절: 묵시적 병렬성에 대해 설명
- 4.3절: 명시적 병렬성의 형태 확인
- 이후 다양한 병렬 프로그래밍 기법을 살펴봄
- 마지막으로 SIMD 벡터 프로세싱과 GPU 병렬 프로그래밍(GPGPU) 적용 사례 분석

---

## 4.2 묵시적 병렬성

- 단일 스레드 실행 속도를 향상시키기 위한 **하드웨어 기반 병렬성 기법**
- CPU 제조사들이 코드 변경 없이 성능을 높이기 위해 채택
- 주요 기법:
  - **파이프라인 (Pipeline)**
  - **슈퍼스칼라 (Superscalar)**
  - **VLIW (Very Long Instruction Word)**

---

### 4.2.1 파이프라인 방식

- 명령어를 여러 단계로 나눠 **겹쳐서 병렬 실행**
- 주요 단계:
  - **인출(Fetch)**: 명령어를 메모리에서 읽음
  - **해석(Decode)**: 명령어 분석
  - **실행(Execute)**: ALU, FPU 등에서 연산 수행
  - **메모리 접근(Memory Access)**: 메모리 읽기/쓰기
  - **레지스터 기록(Write-back)**: 연산 결과를 저장
- 여러 명령어가 동시에 각기 다른 단계에서 실행됨

---

### 4.2.2 지연 시간과 처리량

- **지연 시간 (Latency)**: 명령어 하나가 끝나는 데 걸리는 시간  
  (T_pipeline = 각 단계 지연 시간의 합)

- **처리량 (Throughput)**: 단위 시간당 처리 가능한 명령어 수  
  (처리량 f = 가장 느린 단계의 시간의 역수)

---


### 4.2.3 파이프라인 깊이

- 각 단계의 **지연 시간 균형**이 중요
- 특정 단계가 느리면 전체 성능 저하
- **단계 수를 늘려** 짧은 단계로 분할하여 처리량 개선 가능
- 실제 CPU는 보통 **4~30단계** 사이의 파이프라인 구조를 가짐

---

### 4.2.4 정체 (Stall)

- 다음 명령어가 대기해야 하는 상태
- 파이프라인 단계 중 하나가 **비거나 지연**되면 전체 명령어 흐름이 멈춤
- "거품(bubble)", "딜레이 슬롯(delay slot)"으로도 표현

---


### 4.2.5 데이터 의존성

- 명령어 스트림 내에서 **명령어 간 의존성**으로 발생하는 정체
- 파이프라인의 다섯 단계를 거쳐야 다음 명령어가 실행될 수 있는 경우 발생
- 대표적 예시:
  ```asm
  mov ebx, 5
  imul eax, 10
  add eax, 7  ; imul의 결과가 있어야 add 실행 가능
  ```

---

### 데이터 의존성의 종류

- **데이터 의존성 (Data Dependency)**
- **제어 의존성 (Control Dependency)**
- **구조적 의존성 (Structural Dependency)**

---

#### 4.2.5.1 명령어 재배열

- 의존성 대기 시간 동안 **의존성 없는 명령어를 앞당겨 실행**
- 컴파일러가 자동으로 재배열하거나, 숙련된 프로그래머가 직접 가능
- 성능 개선에 큰 역할, 다중 스레드 레벨에서도 활용됨

---

#### 4.2.5.2 비순차적 명령어 실행 (Out-of-Order Execution)

- CPU가 실행 시점을 재조정하여 **의존성 없는 명령어를 먼저 실행**
- Look-ahead window로 미래 명령어를 탐색
- 정적인 재배열보다 더 강력한 **동적 스케줄링**

---

### 4.2.6 분기 의존성

- **조건 분기 명령어로 인해 발생하는 의존성**
- 분기 결과가 나와야 이후 명령어 실행 가능
- 예시:
  ```c
  return (b != 0) ? a / b : defaultVal;
  ```

---

#### 4.2.6.1 추측 실행 (Speculative Execution)

- 분기 결과를 **예측**하고, 예측된 분기에 따라 명령어 실행
- 예측 실패 시 다시 실행 (branch penalty 발생)
- 고급 CPU는 **분기 예측기** 포함

---

#### 4.2.6.2 프레디케이션 (Predication)

- 조건 분기에 따라 두 결과 중 하나 선택
- **비트 마스크 연산**으로 두 결과 중 하나를 선택
- 분기를 제거해 **분기 예측 실패 가능성 감소**

- 이를 마스크와 union을 사용해 실행 시 예외 없이 처리 가능
- PowerPC, PS3, SIMD 명령어 등에서 활용

### 4.2.7 슈퍼스칼라 CPU

- 기존 파이프라인 구조를 확장해 **한 사이클에 여러 명령어 실행 가능**
- CPU 내부에 각 파이프라인 단계를 담당하는 회로를 **2개 이상 배치**
- 명령어 스케줄러가 **비순차적 실행**과 **추측 실행**을 통해 성능 향상
- 데이터 의존성, 분기 의존성, 자원 의존성 문제 발생 가능

---

#### 4.2.7.1 디자인의 복잡도

- 단순 복붙 아님. 제어 로직이 매우 복잡
- 같은 연산 유닛 자원을 공유할 때 충돌 발생 가능 (자원 의존성)
- 명령어 분배를 위한 로직이 **스칼라 CPU보다 훨씬 복잡**

---

#### 4.2.7.2 슈퍼스칼라와 RISC

- 슈퍼스칼라는 보통 **RISC 기반**으로 설계
- 복잡한 명령 줄이고 트랜지스터 수 절약, 실행 효율 증가

---

### 4.2.8 VLIW (Very Long Instruction Word)

- **명령어 스케줄링을 컴파일러가** 수행 → 하드웨어 단순화
- **복잡한 분배 로직 불필요**, 대신 명령어를 병렬로 묶어서 실행
- CPU는 지정된 유닛에서 **병렬로 동시에 실행**
- 실행 유닛 활용률이 높음 (단, 트랜지스터 수 많아짐)

---

- **PS2, PS3 등 콘솔**에 실제 사용됨 (예: VU0, VU1 벡터 유닛)
- 명령어 묶음 단위로 명확하게 병렬 실행
- 단점: 하드웨어는 단순하지만 **컴파일러 설계는 어려움**

---

## 4.3 명시적 병렬성

- **병행 소프트웨어**를 더 효율적으로 실행하기 위한 구조
- 병렬 하드웨어가 여러 명령어 스트림을 동시에 실행할 수 있게 설계됨
- 단위 크기 기준으로 설명: 하이퍼스레딩 → 멀티코어 → 대칭/비대칭 멀티프로세싱 → 분산 컴퓨팅

---

### 4.3.1 하이퍼스레딩 (Hyper-Threading)

- 하나의 코어에서 **두 개의 스레드 실행** 가능
- 공유 자원(백엔드, L1 캐시)을 활용해 **유휴 슬롯 활용**
- 듀얼코어 대비 실행량은 적지만, 트랜지스터 소모는 작음

---

### 4.3.2 멀티코어 CPU

- 하나의 다이(die)에 **여러 개의 코어** 탑재
- 각 코어가 독립적으로 명령어 스트림 처리
- 다양한 구조와 혼합 가능: 파이프라인, 슈퍼스칼라, VLIW, 하이퍼스레딩 등

#### 콘솔 예시:
- **PS4**: AMD Jaguar 8코어 + Radeon GPU (8GiB GDDR5)
- **Xbox One**: AMD Jaguar 8코어 + Radeon GPU (8GiB GDDR3 + eSRAM)

---

### 4.3.3 대칭 vs 비대칭 멀티프로세싱

- **SMP (Symmetric MultiProcessing)**:
  - 모든 코어 동일, 운영체제가 균등하게 배분
  - 특정 스레드가 특정 코어에서 실행되도록 설정 가능

- **AMP (Asymmetric MultiProcessing)**:
  - 마스터 코어가 나머지를 제어
  - **PS3의 셀 프로세서**가 대표적 예시 (PPU + SPU)

---

### 4.3.4 분산 컴퓨팅

- 여러 독립된 컴퓨터들이 **협업하여 하나의 작업을 처리**
- 가장 넓은 의미의 병렬성
- 예시:
  - 컴퓨터 클러스터
  - 그리드 컴퓨팅
  - 클라우드 컴퓨팅

  ---

 ### 4.4 운영체제 기초

운영체제는 병렬 프로그래밍이 가능한 환경을 제공하는 소프트웨어 핵심 구성 요소.

---

### 4.4.1 커널

- 운영체제의 **핵심** 역할, 하드웨어와 직접 통신
- 키보드, 마우스 등 이벤트 처리 및 프로그램 스케줄링 담당
- 사용자 프로그램은 커널 위에서 실행됨

#### 커널 모드 vs 사용자 모드

- **커널 모드**: 특권 명령 실행 가능 (I/O, 메모리 접근 등)
- **사용자 모드**: 제한된 권한, 커널 호출 통해 로우레벨 서비스 사용
- 보호 고리(protection ring): 신뢰도에 따라 0~3 고리로 나뉨

#### 커널 모드 특권

- 커널 모드는 privileged instruction 사용 가능
- 메모리 보호, 레지스터 설정, 인터럽트 제어 등의 권한 보유

---

### 4.4.2 인터럽트

- CPU에게 **이벤트 발생 알림** (키보드 입력, 타이머 종료 등)
- 하드웨어/소프트웨어 인터럽트로 구분됨
- 인터럽트 서비스 루틴(ISR) 호출 → 현재 작업 중단하고 처리

---

### 4.4.3 커널 호출

- 사용자 프로그램이 커널 기능 요청 시 사용
- 시스템 호출(system call)이라고도 함
- 예: 메모리 매핑, 파일 입출력, 네트워크 접근 등

---

### 4.4.4 선점형 멀티태스킹

- 프로그램 간 **타임 슬라이스**로 CPU를 공유
- 초기에는 협력형(CPU 양보) 방식 → 후에 선점형 도입
- 프로그램이 강제로 중단될 수 있어 안정성 증가

---

### 4.4.5 프로세스

- 실행 중인 프로그램의 **인스턴스**
- 하나의 시스템에 여러 프로세스가 동시 구동 가능

#### 4.4.5.1 프로세스의 구성

- 프로세스 ID (PID)
- 사용자/그룹 권한
- 부모 프로세스 정보
- 가상 메모리 공간
- 환경 변수, 핸들, 작업 디렉터리
- 동기화 및 통신 자원
- **하나 이상의 스레드**

---

- 스레드는 명령어 스트림의 실행 단위
- 하나의 프로세스에 여러 스레드 포함 가능
- 커널은 스레드 단위로 스케줄링함


---

#### 4.4.5.2 프로세스의 가상 메모리 맵

- 프로세스는 물리 주소가 아닌 **가상 주소(virtual address)** 를 통해 메모리에 접근
- 운영체제는 **페이지 테이블**을 이용해 가상 주소를 물리 주소로 매핑
- 각 프로세스는 **고유한 가상 페이지 테이블**을 갖고 있어 독립적인 메모리 공간을 사용

---

### 프로세스의 메모리 맵 구성

- 텍스트, 데이터, BSS 섹션: 프로그램 실행 파일에서 읽혀온 데이터
- 공유 라이브러리(DLL, PRX 등) 관련 정보
- 각 스레드의 콜 스택
- `malloc()` 등으로 할당한 동적 메모리(힙)
- 메모리 맵트 파일 등으로 매핑된 파일 내용
- 접근 불가능한 커널 영역

---

### 텍스트, 데이터, BSS 섹션

- 프로세스 실행 시 커널이 생성한 가상 주소 맵에 매핑
- 텍스트, 데이터, BSS는 낮은 주소부터 연속적으로 배치됨

---

### 콜 스택

- 스레드마다 별도 존재
- 호출이 깊어질수록 **높은 주소에서 낮은 주소 방향**으로 쌓임

---

### 힙 영역

- `malloc()`, `new` 등 동적 메모리 할당에 사용
- 낮은 주소에서 높은 주소 방향으로 증가
- 할당 및 해제에 따라 조정됨

---

### 공유 라이브러리

- 처음 호출한 프로세스가 로드하면 해당 물리 메모리에 적재
- 이후 프로세스들은 이 메모리를 **가상 주소 공간에 매핑**만 수행
- 메모리 절약 및 실행 속도 향상
- 주의사항:
  - 라이브러리 업데이트 시 호환성 문제 발생 가능
  - 윈도우에서는 DLL 지옥 문제 발생 → **매니페스트 시스템**으로 해결

---

### 커널 페이지

- 사용자 공간과는 별도로 존재 (32비트 기준: 0x80000000 이상 영역)
- 사용자 프로세스는 접근 불가
- 시스템 호출 시 커널 모드로 전환하여 접근
- 최근에는 보안 이슈(Meltdown, Spectre 등)로 커널 공간 보호 강화

---

### 프로세스 메모리 맵 예시 (32비트 Windows)

- 텍스트, 데이터, BSS는 낮은 주소에 위치
- 힙은 그 위쪽, 콜 스택은 높은 주소부터 아래로 쌓임
- 공유 메모리는 힙과 스택 사이에 위치
- 커널 공간은 0x80000000 이상 주소부터 예약됨

---

### 4.4.6 스레드

- 스레드는 실행 중인 1개의 기계어 명령어 스트림을 내포
- 프로세스 내의 각 스레드는 다음과 같은 요소로 이루어짐:

  - **스레드 식별자(TID)**  
    프로세스 내에서 고유한 값이며, 운영체제 전체에서 고유하지 않을 수 있음

  - **콜 스택(Call Stack)**  
    현재 실행 중인 함수의 스택 프레임을 포함하는 연속된 메모리 블록

  - **레지스터**  
    명령어 스트림에서 현재 명령어를 가리키는 명령어 포인터(IP), 베이스 포인터(BP), 스택 포인터(SP) 등 포함

  - **스레드 로컬 저장소**  
    각 스레드마다 할당되는 범용 메모리

---

### 기본 개념

- 기본적으로 하나의 프로세스는 하나의 메인 스레드를 가지며, 보통 `main()` 함수부터 실행됨
- 최신 운영체제에서는 하나의 프로세스 안에 여러 개의 스레드 실행이 가능함 (concurrent)

---

### 스레드와 실행 문맥

- 스레드는 명령어 스트림을 실행하기 위한 최소한의 자원(스택 프레임, 레지스터 집합)을 제공
- 프로세스는 이러한 자원을 다수의 스레드에 할당
- 프로세스는 실행 문맥과 가상 메모리 공간을 공유하며, 스레드는 개별 실행 문맥을 가짐

---

#### 4.4.6.1 스레드 라이브러리

- 운영체제는 다중 스레드를 생성/조작하기 위한 다양한 **스레드 라이브러리**를 제공
- 대표적인 표준:
  - POSIX `pthread`
  - C11/C++11 `std::thread`
- 예: 소니 PlayStation SDK의 `sce` 계열 API는 POSIX 스레드와 유사함

---

### 스레드 API의 주요 기능

1. **생성**: 새 스레드를 만드는 함수 또는 클래스 생성자  
2. **종료**: 호출한 스레드를 종료  
3. **종료 요청**: 다른 스레드에게 종료 요청  
4. **휴면**: 일정 시간 동안 현재 스레드를 대기 상태로 전환  
5. **양보**: 현재 스레드의 실행 시간을 다른 스레드에 양보  
6. **결합**: 현재 스레드를 종료 상태로 전환하고, 다른 스레드의 종료를 기다림

---

#### 4.4.6.2 스레드 생성과 종료

- **시작**: 프로그램 실행 시 `main()` 함수에서 시작  
- **생성 함수 예시**:
  - POSIX: `pthread_create()`
  - Windows: `CreateThread()`
  - C++11: `std::thread`

---

### 스레드 종료 방식

- **자연스럽게 종료**: 시작 함수가 리턴될 때 자동으로 종료  
- **명시적 종료**: `pthread_exit()` 같은 함수를 사용하여 종료  
- **다른 스레드에 의해 종료**: 외부에서 강제로 취소 요청  
- **프로세스 종료에 따라 종료**: 프로세스 종료 시 소속된 모든 스레드 종료

---
### 4.4.6.3 스레드 결합

- **여러 자식 스레드를 생성해 계산 작업을 병렬 수행**하고, 이후 메인 스레드에서 결과를 확인하는 상황 예시.
- 예: 1000개의 계산을 4개 스레드로 분할 처리.

```c
ComputationResult g_aResult[1000];

void Compute(void* arg)
{
    uintptr_t startIndex = (uintptr_t)arg;
    uintptr_t endIndex = startIndex + 250;
    for (uintptr_t i = startIndex; i < endIndex; ++i)
    {
        g_aResult[i] = ComputeOneResult(...);
    }
}

void main()
{
    pthread_t tid[4];
    for (int i = 0; i < 4; ++i)
    {
        const uintptr_t startIndex = i * 250;
        pthread_create(&tid[i], nullptr, Compute, (void*)startIndex);
    }

    // 다른 작업 수행...

    // 모든 스레드가 종료될 때까지 기다림
    for (int i = 0; i < 4; ++i)
    {
        pthread_join(&tid[i], nullptr);
    }

    // 체크섬 계산
    unsigned checksum = Sha1(g_aResult, 1000 * sizeof(ComputationResult));
}
```

---

### 4.4.6.4 폴링, 블로킹, 양보

스레드가 어떤 **미래의 조건을 기다릴 때** 사용할 수 있는 3가지 방식:

1. **폴링 (Polling)**  
2. **블로킹 (Blocking)**  
3. **양보 (Yielding)**

---

#### 1. 폴링 (Polling)

- 특정 조건이 충족될 때까지 **짧은 루프 반복**
- 예: `while (!CheckCondition()) { /* 대기 */ }`
- 장점: 단순 구현  
- 단점: CPU 자원을 지속 소모  
- 이 방식은 `spin-wait`, `busy-wait` 라고도 부름

```c
while (!CheckCondition())
{
    // 바쁨-대기 상태로 루프 반복
}
```

---

#### 2. 블로킹 (Blocking)

- 조건이 충족될 때까지 **스레드를 수면(sleep) 상태로 전환**
- CPU 자원을 낭비하지 않음
- 스레드가 블로킹되는 대표적 경우:

  - **파일 열기**: `fopen()` 등
  - **명시적 수면**: `usleep()`, `Sleep()`, `std::this_thread::sleep_until()`
  - **스레드 결합**: `pthread_join()`
  - **뮤텍스 락 대기**: `pthread_mutex_wait()` 등

---

#### 3. 양보 (Yield)

- 폴링 루프 도중 **타임 슬라이스를 포기**
- 조건이 충족될 때까지 `pthread_yield()` 등으로 CPU를 다른 스레드에 넘김

```c
while (!CheckCondition())
{
    // 타임 슬라이스 양보
    pthread_yield(nullptr);
}
```

- `Sleep(0)`, `SwitchToThread()`(Windows), `_mm_pause()`(Intel) 등도 유사 기능

```c
while (!CheckCondition())
{
    // SSE2 전용: CPU 전력 소모 줄이고 대기
    _mm_pause();
}
```

---

### 참고 자료

- https://software.intel.com/en-us/comment/1134767  
- http://software.intel.com/en-us/forums/topic/309231  

### 4.4.6.5 문맥 교환 (Context Switch)

커널이 관리하는 모든 스레드는 다음의 세 상태 중 하나에 있음:

- **실행 중인 상태**: 스레드가 코어에서 실행 중
- **실행 가능한 상태**: 실행 가능하지만 슬라이스(CPU는 한 번에 하나의 쓰레드만 실행할 수 있다. 이때 커널은 각 쓰레드에게 번갈아가면서 CPU를 이용할 시간을 준다 이것을 타임 슬라이스라고한다.)를 기다리는 상태
- **블로킹된 상태**: 수면 상태이며, 조건을 기다리는 중

**문맥 교환**은 커널이 스레드의 상태를 바꿀 때 발생하며, 다음 상황에서 일어남:

- 인터럽트 도중 커널 진입
- 실행 중인 스레드가 명시적으로 커널 호출
- 특정 조건 충족으로 수면 상태의 스레드가 '깨움' 상태로

**스레드의 실행 문맥 (execution context)**:

- 명령어 포인터(IP), 스택 포인터(SP), 베이스 포인터(BP)
- 레지스터(GPR), 함수 호출 스택, 로컬 변수 등 포함

실행 중 → 실행 가능/블로킹 상태 전환 시 **레지스터 상태 저장**  
실행 가능 → 실행 중 전환 시 **레지스터 상태 복원**

**콜 스택**은 가상 메모리 내에 이미 존재하므로 별도 저장 불필요  
단, 프로세스 간 교환 시에는 가상 메모리 맵 등의 정보도 교체해야 함

---

### 4.4.6.6 스레드 우선순위와 선호도

스레드의 **우선순위(priority)** 와 **선호도(affinity)** 는 커널의 스케줄링 방식에 영향을 줌

#### 우선순위 (Priority)

- 높은 우선순위 → 낮은 우선순위보다 먼저 스케줄
- Windows: 6개 클래스 × 7개 레벨 → 총 42개 우선순위

**기본 원칙**:  
우선순위 높은 실행 가능 스레드가 있을 경우  
→ 낮은 우선순위 스레드는 스케줄되지 않음

> starvation(기아 상태) 방지 위해 스케줄러에 외부 정책 도입 가능

#### 선호도 (Affinity)

- 스레드를 특정 코어에 묶거나 선호 요청
- CPU 캐시 친화도 등의 이유로 설정

---

### 4.4.6.7 스레드 로컬 저장소 (Thread Local Storage, TLS)

- 프로세스의 모든 스레드는 자원(메모리)을 공유  
- 단, **TLS는 각 스레드에 독립적 메모리 공간 제공**

#### 예시

- 각 스레드마다 전용 힙 할당자 사용 가능
- TLS는 스레드 실행 문맥의 일부로 동작

#### 구현 방식

- TLS 블록은 프로세스의 모든 스레드가 접근 가능  
- 운영체제는 스레드마다 **자신만의 TLS 블록 주소를 보유**

---

### 4.4.6.8 스레드 디버깅

현대 디버거는 **멀티스레드 디버깅 도구**를 제공함

- 예: **Visual Studio의 스레드 창**
  - 모든 스레드 리스트 확인 가능
  - 각 스레드 클릭 시 실행 문맥 활성화
  - **콜 스택 추적** 및 **로컬 변수 조회**

해당 기능은 실행 가능, 블로킹 상태에서도 작동

---
## 4.4.7 파이버 (Fiber)

### 개념

- 커널 스케줄링이 아닌, 사용자 수준 협력적 멀티태스킹
- 일반적인 스레드는 커널이 스케줄링하지만, 파이버는 **명시적으로 제어**
- **잡(jobs)** 을 효율적으로 처리하고 싶을 때 사용됨 (게임 엔진 등에서 유용)
- 일부 OS에서 지원되며, **Windows**, **PlayStation SDK** 등에서 사용 가능

---

### 4.4.7.1 파이버 생성과 파괴

#### 변환과 생성

- 모든 프로세스는 기본적으로 **1개의 스레드**로 시작됨
- `ConvertThreadToFiber()`로 기존 스레드를 파이버로 변환
- `CreateFiber()`로 새로운 파이버 생성, 시작 주소를 인자로 받음

#### 실행과 파괴

- `SwitchToFiber()`로 파이버 간 전환
- 더 이상 필요 없는 파이버는 `DeleteFiber()`로 제거

---

### 4.4.7.2 파이버 상태

#### 두 가지 상태

- **활성화(active)**: CPU 코어에서 실행 중
- **비활성화(inactive)**: 대기 상태, CPU 리소스를 사용하지 않음

#### 상태 전환

- `SwitchToFiber()` 호출로 현재 파이버는 비활성화, 대상 파이버는 활성화됨
- **오직 해당 방식으로만 상태 전환 가능**

#### 유의점

- 파이버는 **블로킹 상태가 될 수 없음**
- 조건 기다릴 땐 `SwitchToFiber()`를 통해 직접 **양보해야 함**

---

### 4.4.7.3 파이버 이동

- 파이버는 **스레드 간 이동도 가능**
- 예: F 스레드의 파이버 D → G로 전환, 이후 H 파이버가 B 스레드로 이동
- 단, 파이버 간 이동은 **비활성 상태일 때만 가능**

---

### 4.4.7.4 파이버 디버깅

- 파이버는 OS 기능이므로 디버깅 도구에서 **스레드처럼 추적 가능**
- 예: **Visual Studio + SN Systems 디버거(PS4 디버깅용)**
- **콜 스택 추적**, **변수 보기**, **호출 스택 탐색** 등 가능

#### 실전 팁

- 타겟 플랫폼이 파이버 디버깅을 **지원하는지 사전 조사 필요**
- 디버깅 지원 없으면 **도입 장벽** 상승

---

### 4.4.7.5 참고 링크

- [MSDN 문서 링크](https://msdn.microsoft.com/en-us/library/windows/desktop/ms682661(v=vs.85).aspx)

---
### 4.4.8 사용자 레벨 스레드와 코루틴 (User-level threads and Coroutines)

*   **개념**: 커널이 제공하는 스레드와 파이버와 달리, 사용자 레벨 스레드와 코루틴은 **비싼 커널 호출 없이** 여러 독립적인 제어 흐름을 코드로 구현할 수 있는 **가벼운 대안**입니다.
*   **특징**:
    *   완전히 사용자 공간 내에서 구현되며, 운영체제는 이들의 존재를 모릅니다.
    *   각 사용자 레벨 스레드는 일반적인 자료 구조를 통해 표현되며, 스레드 식별자, 이름, 그리고 실행 문맥(CPU 레지스터와 콜 스택 내용) 등의 정보가 담깁니다.
    *   사용자 레벨 스레드 라이브러리는 스레드 생성, 파괴 및 스레드 간 문맥 교환을 지원하는 API 함수를 제공합니다.
    *   이들은 운영체제가 지원하는 '진짜' 스레드 및 파이버의 문맥 내에서 동작합니다.
*   **문맥 교환의 핵심**: CPU 레지스터 값 교체로 귀결되며, 어셈블리어 코드를 통해 효율적으로 구현할 수 있습니다.
*   **C/C++ 지원**:
    *   C/C++ 표준에서 사용자 레벨 스레드를 직접적으로 광범위하게 지원하지는 않지만, 플랫폼에 따라 `ucontext.h` (현재는 권장되지 않음)나 C++ Boost 라이브러리에서 관련 기능을 제공합니다.
*   **코루틴 (Coroutines)**:
    *   사용자 레벨 스레드의 특정 형태로, **비동기 프로그래밍** (예: 웹 서버, 게임)에 매우 유용합니다.
    *   **서브루틴의 개념을 일반화**한 것입니다. 서브루틴은 호출한 곳에 제어권을 반환하며 종료되지만, 코루틴은 여기에 더해 다른 코루틴에 **제어권을 양보**함으로써 종료할 수도 있습니다.
    *   코루틴이 양보하더라도 **실행 문맥이 메모리에 유지**되며, 다음 호출 시 중단되었던 지점부터 실행이 재개됩니다.
    *   서브루틴 호출이 계층적인 반면, 코루틴은 서로를 **대칭적으로 호출**할 수 있어 무한히 반복적인 패턴을 만들어도 콜 스택 깊이 문제가 발생하지 않습니다. 이는 각 코루틴이 자체 실행 문맥을 유지하기 때문입니다.
    *   사용자 레벨 스레드로 구현되므로 문맥 교환이 **매우 효율적**입니다.
    *   Ruby, Lua, Go와 같은 고수준 언어에서 지원되며, C++에서는 Boost 라이브러리 또는 직접 구현하여 사용할 수 있습니다.

#### 4.4.8.2 커널 스레드 vs 사용자 스레드

*   '스레드'라는 용어는 맥락에 따라 매우 다른 두 가지 개념을 의미할 수 있어 혼란을 초래할 수 있습니다.
*   **리눅스에서의 두 가지 정의**:
    1.  **첫 번째 정의**:
        *   **커널 스레드**: 커널이 내부 용도로 만들며 특권 모드에서만 실행되는 특수한 형태의 스레드.
        *   **사용자 스레드**: 프로세스를 위한 스레드 (예: `pthread_create`, `std::thread` 등으로 생성)로, 프로세스의 문맥 내 사용자 공간에서 동작.
    2.  **두 번째 정의**:
        *   **커널 스레드**: 커널 공간과 사용자 공간 모두에서 실행될 수 있는 스레드.
        *   **사용자 스레드**: 커널에 전혀 연관되지 않고 사용자 공간 프로그램에서만 실행되는 제어 흐름 (예: 코루틴)을 지칭하는 데 사용.
*   **파이버의 모호성**: 파이버는 커널이 인지하고 별도의 상태를 유지하지만, 스케줄링에는 커널이 관여하지 않는다는 점에서 두 번째 정의에 따르면 구분이 모호해집니다.

#### 4.4.9 프로세스와 스레드에 관해 더 읽을거리

*   이 주제에 대한 추가 정보는 다음 웹사이트에서 찾아볼 수 있습니다:
    *   스레드의 기초: `https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/4_Threads.html`
    *   `pthread` 전체 API: 'pthread documentation' 검색
    *   윈도우 스레드 API: `https://msdn.microsoft.com/`에서 'Process and Thread Functions' 검색
    *   스레드 스케줄링: 니키타 이쉬코프(Nikita Ishkov)의 'A Complete Guide to Linux Process Scheduling' 검색
    *   Go의 코루틴 (고루틴) 구현: 롭 파이크(Rob Pike)의 프레젠테이션 `https://www.youtube.com/watch?v=fokdp27TYZs`

### 4.5 병행 프로그래밍 입문 (Introduction to Concurrent Programming)

*   **핵심**: 병렬 컴퓨팅 하드웨어의 이점을 활용하는 방법은 **병행 프로그래밍(concurrent programming)**에 있습니다.
*   **병행 소프트웨어**: 한 문제를 해결하는 데 여러 개의 반 독립적인 제어 흐름으로 나뉘어 처리됩니다. 중요한 것은 **공유 데이터에 대한 여러 읽기 및 쓰기 동작이 연관**되어야 한다는 점입니다.
*   **정의 (롭 파이크)**: 병행성은 '독립적으로 실행하는 연산의 조합'으로 정의됩니다. 이는 병행 시스템의 제어 흐름이 반 독립적으로 동작하지만, 데이터 공유와 다양한 동기화를 통해 조합된다는 개념을 강조합니다.
*   **다양한 형태의 병행성**:
    *   리눅스와 윈도우 환경에서 파이프로 연결된 여러 프로그램 (예: `cat render.cpp | grep "Light"`).
    *   단일 프로세스 내에서 가상 메모리 공간을 공유하고 동일한 데이터에 동작하는 여러 스레드.
    *   장면 렌더링을 위해 GPU에서 동작하는 수천 개의 스레드 그룹.
    *   여러 대의 PC나 게임 콘솔에서 동일한 게임 상태를 공유하는 멀티플레이어 비디오 게임.

#### 4.5.1 왜 병행 소프트웨어를 짜야 할까?

*   병행 프로그램을 작성하는 주된 이유는 문제를 표현하는 데 있어 하나의 제어 흐름보다 **여러 개의 반독립적인 제어 흐름을 통한 구현이 더 적합**할 때가 있기 때문입니다.
*   때로는 순차적인 디자인이 더 자연스러울지라도 **멀티코어 플랫폼을 최대한 활용**하기 위해 병행적 디자인을 채택하기도 합니다.

#### 4.5.2 병행 프로그래밍 모델 (Concurrent Programming Models)

*   병행 프로그램 내의 여러 스레드가 함께 일하기 위해서는 **데이터를 공유**하고 **서로 동기화**하여 통신할 수 있어야 합니다.
*   **스레드 간 통신 방식**:
    1.  **메시지 전달 (Message Passing)**:
        *   병행 스레드들이 데이터를 공유하거나 행동을 동기화하기 위해 **메시지를 주고받는 방식**입니다.
        *   네트워크, 프로세스 간 파이프, 또는 공유 메모리에 있는 메시지 큐 등을 통해 구현될 수 있습니다.
        *   **한 컴퓨터 내의 스레드** 간 통신뿐만 아니라 **물리적으로 구분된 컴퓨터** (예: 컴퓨터 클러스터) 간 통신에도 사용될 수 있습니다.
    2.  **공유 메모리 (Shared Memory)**:
        *   2개 이상의 스레드가 **동일한 물리 메모리 블록에 접근 권한**을 갖고 해당 메모리 내의 데이터 객체를 직접 접근하는 통신 모델입니다.
        *   이 방식은 모든 스레드가 한 컴퓨터 내에서 동작하고, 모든 CPU 코어가 동일한 물리 RAM 뱅크에 접근할 수 있을 때만 가능합니다.
        *   동일한 프로세스의 스레드는 항상 가상 주소 공간을 공유하므로 공유 메모리를 사용할 수 있습니다.
        *   서로 다른 프로세스의 스레드들은 특정 물리 메모리 페이지를 각자 프로세스의 가상 주소 공간에 매핑함으로써 공유 메모리를 사용할 수 있습니다.
        *   **장점**: **큰 데이터를 공유하는 데 매우 효율적**입니다. 데이터 복사가 필요 없기 때문입니다.
*   **상호 관계**: 메시지 전달 시스템은 공유 메모리 구조를 기반으로 구현될 수 있고, 반대로 공유 메모리 모델도 메시지 전달 메커니즘으로 구현될 수 있습니다.
*   **동기화 문제**:
    *   공유 메모리 모델은 **자원을 공유하는 온갖 동기화 문제**를 동반하며, 이 문제들은 추론하기 어렵고 정확성을 보장하기 까다롭습니다.
    *   메시지 전달 디자인은 이러한 동기화 문제의 영향을 상대적으로 적게 받습니다 (완전히 자유로운 것은 아닙니다).
*   **이 책의 중점**: 공유 메모리를 통한 병행성에 중점을 둡니다. 이는 게임 프로그래밍에서 가장 흔하며, 이해하기 더 어려운 개념이기 때문입니다.

#### 4.5.3 경쟁 상태 (Race Condition)

*   **정의**: 프로그램의 행동이 **타이밍에 의존적인 상황**을 가리킵니다. 즉, 여러 작업에 투입되는 시간이 변하거나 시스템 내의 사건 순서가 변경됨으로써 프로그램의 행동이 달라질 수 있습니다.

#### 4.5.3.1 치명적 경쟁 (Critical Race)

*   경쟁 상태 중에는 아무 해가 없는 경우도 있지만, **치명적 경쟁(critical race)** 은 경쟁 상태에 의해 프로그램의 행동이 **올바르지 않게** 될 가능성이 존재할 때를 뜻합니다.
*   치명적 경쟁으로 발생하는 버그는 경험 없는 프로그래머에게 '이상하게' 보이거나 '절대 발생할 수 없는' 것처럼 보이기도 합니다.
*   **징후**:
    *   때때로 발생하거나 완전히 랜덤하게 발생하는 버그 또는 크래시.
    *   올바르지 않은 결과물.
    *   자료 구조가 오염됨.
    *   디버그 빌드로 실행하면 감쪽같이 사라지는 버그.
    *   한동안 보이다가 며칠 동안 사라지고, 갑자기 다시 나타나는 버그 (특히 데드라인 전).
    *   문제 원인을 찾기 위해 코드를 수정하면 (예: `printf()` 디버깅) 없어지는 버그.
*   프로그래머들은 이런 이슈를 **하이젠버그(Heisenbug)** 라고 부릅니다.

#### 4.5.3.2 데이터 경쟁 (Data Race)

*   **정의**: **치명적 경쟁 상태의 일종**으로, 2개 이상의 제어 흐름이 **공유 데이터**를 읽고 쓰는 과정에서 **서로 간섭**하여 **데이터 오염**으로 이어지는 상황을 뜻합니다.
*   **해결**: 병행 프로그램을 작성하는 것은 언제나 **데이터 경쟁을 찾아내고 제거**하는 행위로 귀결됩니다. 이는 공유 데이터 접근을 세심하게 관리하거나, 공유 데이터를 각자의 독립된 복제 데이터로 대체하는 방식(순차 프로그램으로 치환)으로 처리됩니다.
*   **예시 (공유 카운터 `g_count`)**:
    *   `++g_count;`는 `Read-Modify-Write(RMW)` 동작입니다.
    *   두 스레드 A, B가 동시에 `IncrementCount()` 함수를 호출할 때, 정상적인 기대값은 `g_count`가 2가 되는 것입니다.
    *   **문제 발생 시나리오 (단일 코어 선점형 멀티태스킹)**: 스레드 A가 `mov` 명령어를 실행한 후 스레드 B로 문맥 교환이 일어나고, 스레드 B가 `mov`, `inc`, `mov`를 완료합니다. 그 후 스레드 A로 다시 문맥 교환이 일어나 `inc`, `mov`를 완료하면, `g_count`의 최종 값은 1이 되어 우리가 기대하는 2가 되지 않습니다.
    *   **문제 발생 시나리오 (병렬 하드웨어)**: 두 스레드가 동시에 `g_count` 값을 자신의 레지스터로 읽어오고, 각자 값을 증가시킨 후 메모리에 다시 저장하면, 한 스레드의 동작이 다른 스레드의 값을 덮어써서 최종 값이 1이 되는 문제가 발생할 수 있습니다. 이는 두 스레드가 완전히 동기화된 상태로 실행될 때도 발생할 수 있습니다.

#### 4.5.4 임계 동작과 원자성 (Critical Operations and Atomicity)

*   **임계 동작 (Critical Operation)**: 특정 **공유 데이터 객체**에 대해 수행되는, **읽거나 수정할 수 있는 모든 동작**을 임계 동작이라고 부릅니다.
*   **원자 동작 (Atomic Operation)**: 공유 객체가 데이터 경쟁 버그로부터 자유로우려면 해당 데이터에 대한 모든 임계 동작이 다른 임계 동작에 의해 끊겨서는 안 됩니다. 이와 같이 **끊길 수 없게 구현된 임계 동작**을 **원자 동작** 이라고 하며, 이런 동작은 **원자성 (Atomicity)** 을 가졌다고 말할 수 있습니다.

#### 4.5.4.1 호출과 응답 (Invocation and Response)

*   병행 시스템에서는 **즉시 발생 사건(instantaneous event)**에 한해서만 순서를 정의할 수 있습니다.
*   일정한 실행 시간이 걸리는 동작의 경우, 두 개의 즉시 발생 사건으로 분해할 수 있습니다:
    *   **호출 (invocation)**: 동작 수행이 시작되는 순간.
    *   **응답 (response)**: 동작이 완료됐다고 생각되는 순간.
*   **코드 구역**: 특정 공유 데이터 객체에 임계 동작을 수행하는 코드는 세 구역으로 나눌 수 있습니다:
    *   **전위 구역 (pre-section)**: 프로그램 순서상 임계 동작의 호출 이전에 실행되는 모든 코드.
    *   **임계 구역 (critical section)**: 임계 동작을 구성하는 코드.
    *   **후위 구역 (post-section)**: 프로그램 순서상 임계 동작의 응답 이후에 실행되는 모든 코드.

#### 4.5.4.2 원자성에 대한 정의 (Definition of Atomicity)

*   데이터 경쟁은 특정 객체에 대한 임계 동작이 같은 객체에 대한 다른 임계 동작에 의해 중단될 때 발생합니다.
*   **원자적 실행 (Atomic Execution) 정의**: **임계 동작의 호출과 응답이 같은 객체에 대한 다른 임계 동작에 의해 중단되지 않고 실행**됐을 경우, 이 동작은 원자적으로 실행됐다고 말할 수 있습니다.
*   **중요**: 임계 동작이 비 임계 동작이나 다른 데이터 객체에 대한 임계 동작에 의해 중단되는 것은 아무런 문제가 되지 않습니다. 오직 같은 객체에 대한 두 개의 임계 동작이 서로를 중단시킬 때만 데이터 경쟁 버그가 발생합니다.
*   이론적으로는 시스템의 모든 스레드가 보기에 임계 동작이 **순간적으로 수행되는 것처럼 보이도록** 할 수 있다면 이 동작이 원자적으로 실행된다는 것을 보장할 수 있습니다.

#### 4.5.4.3 원자적 동작 만들기 (Making Operations Atomic)

*   가장 흔한 방법은 **뮤텍스(mutex)** 라고 불리는 특수 객체를 사용하는 것입니다.
*   **뮤텍스**: 운영체제가 제공하는 객체로, **자물쇠 역할**을 하며 스레드가 잠그고 열 수 있습니다.
*   **원리**: 특정 공유 데이터 객체에 대한 두 임계 동작이 있을 때, 각 동작의 호출 지점에서 뮤텍스를 획득하고, 응답 지점에서 뮤텍스를 반납합니다. 운영체제가 한 번에 한 스레드만 뮤텍스를 획득할 수 있도록 보장하므로, 한 동작의 호출/응답이 다른 동작의 호출/응답 사이에 발생하는 일은 절대 일어날 수 없습니다.
*   뮤텍스는 **스레드 동기화 기본 도구 (thread synchronization primitive)** 의 한 종류입니다.

#### 4.5.4.4 직렬화로서의 원자성 (Atomicity as Serialization)

*   원자성을 부여하면 한 번에 한 스레드만 해당 동작을 수행하도록 **강제**할 수 있습니다. 이는 서로 겹치고 어지럽던 동작들이 질서 정연한 **원자적 동작들의 순차적인 흐름** 으로 바뀌는 효과를 낳습니다.
*   이는 동작들의 순서 자체에는 영향을 주지 못하며, 단지 동작들이 모종의 **순차적인 형태**로 수행된다고 말할 수 있을 뿐입니다.

#### 4.5.4.5 데이터 주도 일관성 모델 (Data-centric Consistency Model)

*   병행 시스템 내에서 원자성의 개념과 동작의 직렬화는 **데이터 주도 일관성 모델(data-centric consistency model)**이라는 더 큰 주제의 일부입니다.
*   **일관성 모델 (consistency model)**: 데이터 저장소(공유 데이터 객체, 데이터베이스 등)와 이 저장소를 공유하는 스레드 집합 간의 계약입니다.
*   이 계약은 데이터 저장소의 행동을 이해하는 데 도움을 주며, 스레드가 계약 규약을 어기지 않는다면 데이터 저장소가 일관되고 예측 가능하도록 동작하며, 데이터가 오염되지 않으리라 확신할 수 있습니다.
*   **선형화 가능 (linearizable)**: 원자성을 보장하는 데이터 저장소를 일컫는 말입니다.
*   더 자세한 내용은 위키피디아에서 'consistency model', 'linearizability'를 검색하거나 관련 문서를 참조할 수 있습니다.

### 4.6 스레드 동기화 기본 도구 (Thread Synchronization Primitives)

*   운영체제는 병행성 개념을 제공하기 위해 **스레드 동기화 기본 도구**라는 일련의 도구들을 제공합니다.
*   **제공 서비스**:
    1.  **자원 공유 지원**: 임계 동작이 **원자적으로 동작**하게 함으로써 스레드 간의 자원 공유를 지원합니다.
    2.  **스레드 간 동작 동기화**:
        *   스레드가 특정 자원을 기다리거나 다른 스레드(들)이 특정 동작을 완료하기를 기다리면서 **수면 상태로 들어갈 수 있도록** 지원합니다.
        *   실행 상태인 스레드가 수면 상태의 다른 스레드(들)에 **신호를 보내 깨울 수 있는** 기능도 지원합니다.
*   **비용**: 이 동기화 도구들은 안정적이고 사용하기 쉽지만, 운영체제가 지원하기 때문에 **상당히 비쌉니다**. 처리를 위해 **커널 호출**과 **보호 모드로의 문맥 전환**이 필요하며, 이는 최대 1000클럭 주기까지 들 수 있습니다.
*   **대안**: 이러한 높은 비용 때문에 프로그래머 중에는 직접 원자성을 구현하거나 **락-프리(lock-free) 프로그래밍**을 시도하여 효율성을 향상시키는 경우도 있습니다.

#### 4.6.1 뮤텍스 (Mutex)

*   **개념**: 운영체제의 객체로, **임계 동작을 원자적으로 만들 수 있게** 하는 도구입니다.
*   **상태**: 잠기지 않은 (해제된, 신호) 상태와 잠긴 (획득된, 비신호) 상태 중 하나입니다.
*   **핵심 특징**: **한 번에 하나의 스레드만 뮤텍스를 획득할 수 있습니다**. 따라서 특정 공유 데이터 객체에 대한 모든 임계 동작이 상호 배타적(mutual exclusion)이 됩니다. '뮤텍스'라는 이름도 여기서 유래했습니다.
*   **기본 API**:
    1.  `create()` 또는 `init()`: 뮤텍스를 생성하거나 초기화.
    2.  `destroy()`: 뮤텍스를 파괴.
    3.  `lock()` 또는 `acquire()`: 스레드에서 뮤텍스를 잠그기 위해 호출하는 **블로킹 함수**. 뮤텍스가 다른 스레드에 의해 잠겨 있을 경우, 호출 스레드를 수면 상태로 전환합니다.
    4.  `try_lock()` 또는 `try_acquire()`: 뮤텍스를 잠그려 시도하는 **논블로킹 함수**. 뮤텍스를 획득할 수 있으면 바로 리턴합니다.
    5.  `unlock()` 또는 `release()`: 뮤텍스의 잠금을 해제하는 **논블로킹 함수**. 대부분의 운영체제에서는 뮤텍스를 잠갔던 스레드에서만 해제할 수 있습니다.
*   **신호/비신호**: 뮤텍스가 어떤 스레드에 의해 잠긴 경우 **비신호 상태**이고, 해제하면 **신호 상태**가 됩니다. 뮤텍스가 신호 상태가 되고 이를 기다리는 스레드가 있으면 커널은 대기 스레드 중 하나를 깨웁니다.

#### 4.6.1.1 POSIX (Portable Operating System Interface)

*   POSIX 스레드 라이브러리는 C 스타일 함수 인터페이스로 커널 뮤텍스 객체를 제공합니다.
*   **예시 (공유 카운터 원자적 처리)**: `pthread_mutex_lock(&g_mutex); ++g_count; pthread_mutex_unlock(&g_mutex);`.

#### 4.6.1.2 C++11 표준 라이브러리

*   C++11부터 `std::mutex` 클래스를 통해 뮤텍스 기능을 지원합니다.
*   **예시**: `g_mutex.lock(); ++g_count; g_mutex.unlock();`.
*   `std::mutex`의 생성자와 소멸자가 뮤텍스의 초기화와 파괴를 처리하여 사용하기가 더 간편합니다.

#### 4.6.1.3 윈도우 (Windows)

*   윈도우 환경에서는 `HANDLE` 타입을 통해 뮤텍스를 나타내고 관련 시스템 호출을 사용합니다.
*   뮤텍스를 잠그는 데는 범용 함수인 `WaitForSingleObject()`를 사용하고, 해제하는 데는 `ReleaseMutex()`를 사용합니다.

#### 4.6.2 크리티컬 섹션 (Critical Section)

*   **개념**: 윈도우에서 제공하는, 뮤텍스를 대신할 수 있는 **다소 가벼운 대안**입니다. 용어와 API는 뮤텍스와 조금 다르지만 윈도우 환경에서는 경량 버전 뮤텍스에 해당합니다.
*   **API**:
    1.  `InitializeCriticalSection()`: 크리티컬 섹션 객체 생성.
    2.  `DeleteCriticalSection()`: 초기화된 크리티컬 섹션 객체 파괴.
    3.  `EnterCriticalSection()`: 호출 스레드에서 크리티컬 섹션을 잠그는 **블로킹 함수**. 잠글 수 없으면 스핀-대기하거나 스레드를 수면 상태로 전환.
    4.  `TryEnterCriticalSection()`: 크리티컬 섹션을 잠그려 시도하는 **논블로킹 함수**. 실패할 경우 바로 리턴.
    5.  `LeaveCriticalSection()`: 크리티컬 섹션 객체의 잠금을 해제하는 **논블로킹 함수**.
*   **효율성 구현**: 다른 스레드가 이미 잠그고 있는 크리티컬 섹션을 잠그려 할 때, 우선 비용이 저렴한 **스핀 락(spin lock)** 을 사용하고, 락을 획득할 수 없을 때만 뮤텍스처럼 스레드를 수면 상태로 전환합니다. 이 방식 덕분에 크리티컬 섹션은 뮤텍스보다 비교적 효율적입니다.
*   **다른 OS**: 리눅스는 윈도우의 크리티컬 섹션과 유사한 '저렴한' 뮤텍스 변종인 `futex`를 제공합니다.

#### 4.6.3 조건 변수들 (Condition Variables)

*   **목적**: 병행 프로그래밍에서는 스레드들 간의 동작을 동기화하고자 **스레드들끼리 신호를 보낼 필요**가 있습니다. 대표적인 예가 생산자-소비자 문제입니다.
*   **전역 불리언 변수 사용의 문제**: 생산자 스레드가 데이터를 생산한 후 전역 불리언 변수를 `true`로 설정하고, 소비자 스레드가 이를 폴링하면서 타이트 루프(바쁜-대기)를 돌게 됩니다. 이는 귀중한 **CPU 주기를 낭비**하게 되는 큰 문제입니다.
*   **해결책**: 생산자가 일하는 동안 소비자 스레드를 블록(수면 상태로 진입)했다가 데이터가 준비됐을 때 깨울 수 있는 방법이 필요하며, 이를 위해 **조건 변수(Condition Variable)** 라는 새로운 커널 객체가 사용됩니다.
*   **개념**: 조건 변수는 실제로 상태를 저장하는 변수가 아닙니다. 사실은 **대기 중인(수면 상태) 스레드의 큐**로, 실행 중인 스레드가 원할 때 이들을 깨울 수 있는 방법이 제공됩니다. 수면 상태 진입과 깨우기는 뮤텍스와 커널의 지원을 통해 원자적으로 수행됩니다.
*   **기본 API**:
    1.  `create()` 또는 `init()`: 조건 변수 생성.
    2.  `destroy()`: 조건 변수 파괴.
    3.  `wait()`: 호출하는 스레드를 수면 상태로 전환하는 **블로킹 함수**.
    4.  `notify()`: 조건 변수를 기다리며 수면 상태인 스레드 하나 이상을 깨우는 **논블로킹 함수**.
*   **생산자-소비자 문제 예시 (조건 변수 사용)**:
    *   소비자 스레드는 `pthread_cond_wait()`를 호출함으로써 특정 조건이 참이 될 때까지 수면 상태로 전환됩니다.
    *   생산자 스레드는 데이터를 생산한 후 조건 변수를 `true`로 만들고 `pthread_cond_signal()`을 호출하여 수면 중인 소비자 스레드를 깨웁니다.
    *   **뮤텍스 잠금 유지**: `pthread_cond_wait()`를 호출할 때 뮤텍스를 잠근 상태로 진입하는 것이 일반적입니다. 스레드가 안전하게 잠들면 커널이 자동으로 뮤텍스 락을 해제해주고, 스레드가 깨어나면 다시 뮤텍스 락을 가질 수 있도록 처리해줍니다.
    *   **스퓨리어스 웨이크업(Spurious Wakeup)**: `pthread_cond_wait()`가 리턴하더라도 조건의 값이 참이 아닐 수도 있으므로, `while (!g_ready)`와 같은 루프를 통해 값을 재확인하는 것이 필요합니다.

#### 4.6.4 세마포어 (Semaphore)

*   **개념**: 뮤텍스가 원자적 불리언 변수라면, 세마포어는 **잠기지 않는 원자적 카운터**와 같은 역할을 합니다. 세마포어는 **하나 이상의 스레드가 동시에 획득할 수 있는** 특수한 형태의 뮤텍스라고 생각할 수 있습니다.
*   **활용**: **제한된 자원들을 여러 스레드 그룹이 공유**하는 데 세마포어를 사용할 수 있습니다 (예: 특정 개수로 제한된 오프라인 버퍼).
*   **기본 API**:
    1.  `init()`: 세마포어 객체를 초기화하고 카운터 값을 정해진 초기 값으로 설정.
    2.  `destroy()`: 세마포어 객체 파괴.
    3.  `take()` 또는 `wait()`: 세마포어가 내포하는 카운터 값이 0보다 큰 경우 값을 감소시키고 즉시 리턴합니다. 카운터 값이 0인 경우 이 함수는 카운터가 0보다 커질 때까지 **블로킹 상태**가 됩니다 (스레드를 수면 상태로 전환).
    4.  `give()`, `post()` 또는 `signal()`: 세마포어가 내포하는 카운터의 값을 1 증가시킵니다. 어떤 스레드가 세마포어를 기다리며 수면 상태인 경우, 이 함수는 해당 스레드를 깨웁니다.
*   **신호/비신호**: 세마포어의 카운터가 0보다 큰 상태를 **신호 상태**라고 하고, 카운터가 0인 경우 **비신호 상태**라고 합니다.

#### 4.6.4.1 계수 세마포어 vs 이진 세마포어 (Counting Semaphore vs Binary Semaphore)

*   **이진 세마포어 (Binary Semaphore)**: 초기값이 1인 세마포어를 이진 세마포어라고 합니다.
*   **뮤텍스와의 차이**: 뮤텍스는 잠금 해제가 뮤텍스를 잠근 스레드에서만 가능하지만, 세마포어는 카운터를 증가시키는 스레드와 감소시키는 스레드가 다를 수 있습니다. 즉, 이진 세마포어는 '잠근' 스레드 아니더라도 '잠금을 푼' 스레드가 다를 수 있습니다.
*   **용도**:
    *   뮤텍스는 동작을 **원자적**으로 만들 때 사용됩니다.
    *   이진 세마포어는 **스레드 간 신호 전달**에 사용됩니다.
*   **생산자-소비자 문제 예시 (세마포어 사용)**: 생산자가 `sem_post()`를 통해 사용된 아이템 카운터(`g_semUsed`)를 증가시켜 소비자에게 데이터 준비를 알리고, 소비자는 `sem_wait()`를 통해 데이터 준비를 기다립니다. 소비자는 아이템을 처리한 후 `sem_post()`를 통해 여유 공간 카운터(`g_semFree`)를 증가시켜 생산자에게 알립니다.

#### 4.6.4.2 세마포어 구현 (Semaphore Implementation)

*   세마포어는 뮤텍스 하나, 조건 변수 하나, 그리고 정수 하나로 구현할 수 있습니다. 이는 세마포어가 뮤텍스나 조건 변수보다 '고차원' 도구임을 의미합니다.

#### 4.6.5 윈도우 이벤트 (Windows Events)

*   윈도우는 조건 변수와 비슷하지만 사용하기는 더 간편한 **이벤트 객체(event object)**를 지원합니다.
*   **활용**: 이벤트 객체가 만들어진 후 `WaitForSingleObject()`를 호출해서 수면 상태로 갈 수 있고, 다른 스레드가 `SetEvent()`를 호출함으로써 깨울 수 있습니다.

### 4.7 락 기반 병행성의 문제점 (Problems with Lock-Based Concurrency)

*   데이터 경쟁 문제를 해결하고 공유 데이터 객체에 대한 동작을 원자적으로 만들기 위해 락을 사용할 수 있지만, 락으로 감싸더라도 병행 시스템에서는 여전히 다른 문제가 발생할 수 있습니다.

#### 4.7.1 데드락 (Deadlock)

*   **정의**: 시스템의 모든 스레드가 더 이상 진행하지 못하는 상황을 뜻합니다. 데드락이 발생하면 모든 스레드가 어떤 리소스에 접근할 수 없기를 기다리며 블로킹 상태에 들어가지만, 실행 가능한 스레드가 없기 때문에 리소스가 해제될 일이 없어 전체 프로그램이 멈춥니다.
*   **발생 조건**: 데드락이 발생하려면 **2개 이상의 스레드**와 **2개 이상의 리소스**가 있어야 합니다.
*   **순환 의존성**: 데드락 상황을 정의하는 핵심은 스레드와 리소스들 간의 **순환적(circular) 의존성**입니다. 이를 그래프로 그렸을 때 순환 구조를 갖는다면 데드락입니다.
*   **코프먼 조건 (Coffman Conditions)**: 데드락이 발생하기 위한 네 가지 필요 조건입니다.
    1.  **상호 배제 (Mutual Exclusion)**: 둘 이상의 스레드가 하나의 리소스에 배타적 접근 권한을 가질 수 있는 상황이 생김.
    2.  **점유와 대기 (Hold and Wait)**: 스레드가 다른 락을 기다리며 수면 상태로 진입할 때 하나 이상의 락을 가진 상태.
    3.  **락에 대한 선점 불가 (No Lock Preemption)**: 수면 상태의 스레드가 가진 락을 강제로 해제할 방법이 없음.
    4.  **순환 대기 (Circular Wait)**: 스레드 의존성 그래프에 순환 구조가 존재.
*   **데드락 회피**: 항상 코프먼 조건 중 하나 이상을 충족시키지 못하게 하는 것으로 귀결됩니다. 일반적으로 규칙 1과 3은 회피하기 어렵기 때문에, 주로 **조건 2 (점유와 대기)와 조건 4 (순환 대기)를 피하는 쪽**으로 해결합니다.
    *   **점유와 대기 회피**: 여러 리소스를 하나의 락으로 감싸서, 락을 획득하는 동안 다른 락을 기다리지 않도록 합니다.
    *   **순환 대기 회피**: 전체 시스템에서 **락을 거는 순서를 통일**하여, 스레드가 락을 획득하는 순서에 대한 강제적인 전역 순서를 설정합니다.

#### 4.7.2 라이브락 (Livelock)

*   **정의**: 데드락 문제에 대한 한 해법으로 스레드가 수면 상태에 들어가지 않은 채 락 획득을 시도하나, **성공하지 못하고 계속해서 락 획득을 시도하는 상황**입니다. 스레드는 CPU 주기를 낭비하며 실제 작업은 진행하지 못합니다.
*   **비유**: 라이브락은 예의 바른 데드락 또는 **도와주는 데드락**에 비유할 수 있습니다.
*   **해결**: 비대칭적 데드락 해소 알고리듬을 사용하면 피할 수 있습니다 (예: 특정 조건에 의해 선정된 스레드만 데드락 감지 시 해소를 시도).
*   **기아 상태와의 관계**: 라이브락은 **기아 상태(starvation)의 일종**이라고 할 수 있습니다.

#### 4.7.3 기아 상태 (Starvation)

*   **정의**: 하나 이상의 스레드가 CPU 시간을 **전혀 얻지 못하는 상황**을 의미합니다.
*   **원인**: 우선순위가 높은 스레드(들)이 CPU에 대한 컨트롤을 놓지 않기 때문에 낮은 우선순위 스레드가 실행될 수 없는 상황에서 발생합니다.
*   **해결**: 높은 우선순위 스레드의 실행 시간을 제한하고, 시스템의 CPU 자원을 낮은 우선순위 스레드들에게 공평하게 분배하도록 하는 것이 중요합니다. 높은 우선순위 스레드는 가능 한 빨리 일을 처리하고 종료하여 다른 스레드들이 CPU 자원을 사용할 수 있게 해야 합니다.

#### 4.7.4 우선순위 역전 (Priority Inversion)

*   **정의**: 뮤텍스 락은 **우선순위 역전(priority inversion)**이라는 특수한 상황을 야기할 수 있습니다. 이는 **낮은 우선순위 스레드가 시스템에서 가장 높은 우선순위를 가진 것처럼 행동하는 상황**을 의미합니다.
*   **발생 시나리오**:
    *   낮은 우선순위 스레드 L이 락을 가져간 후, 높은 우선순위 스레드 H에 의해 선점당합니다. H가 같은 락을 획득하려고 하면 L이 이미 락을 가지고 있기 때문에 H는 수면 상태에 들어가게 됩니다. 이 상황에서는 L이 우선순위가 낮지만 먼저 실행하게 됩니다 (이는 높은 우선순위 스레드가 실행 가능 상태인 경우 낮은 우선순위 스레드가 실행되어서는 안 된다는 원칙에 위배).
    *   중간 우선순위 스레드 M이 이미 락을 가진 L을 선점하는 경우에도 우선순위 역전이 발생할 수 있습니다.
*   **영향**: 사소한 부작용만 일으킬 수도 있지만, 극단적인 경우 데드락이나 다른 시스템 실패를 유발할 수 있습니다. 예를 들어, 높은 우선순위 스레드가 크리티컬한 데드라인을 지키지 못하게 될 수 있습니다.
*   **해결책**:
    *   낮은 우선순위 스레드와 높은 우선순위 스레드가 동시에 가질 수 있는 락을 만들지 않습니다 (현실적으로 어려움).
    *   **우선순위 승계 (priority inheritance)**: 뮤텍스 자체에 매우 높은 우선순위를 부여하여, 이 뮤텍스를 가진 스레드는 획득하려는 스레드의 우선순위와 동등하게 높아지도록 합니다. 따라서 락을 가진 동안 선점당하지 않습니다.
    *   임의적으로 우선순위를 증가시키는 방법도 있습니다 (윈도우 스케줄링 모델에서 사용).

#### 4.7.5 식사하는 철학자들 (Dining Philosophers)

*   널리 알려진 '식사하는 철학자들' 문제는 **데드락, 라이브락, 기아 상태**를 설명하기에 매우 적합한 문제입니다.
*   **문제의 본질**: 원형 테이블에 둘러앉은 5명의 철학자와 그들 앞에 놓인 스파게티 접시, 그리고 각 철학자 사이에 놓인 젓가락 하나 (총 5개)가 있습니다. 철학자들은 생각(젓가락 필요 없음)과 식사(2개의 젓가락 필요) 사이를 번갈아 하고 싶어 합니다. 이 문제의 본질은 철학자들이 데드락, 라이브락, 기아 상태를 겪지 않으면서 생각과 식사를 번갈아 할 수 있는 행동 패턴을 정의하는 것입니다 (철학자는 스레드, 젓가락은 뮤텍스 락에 비유).
*   **일반적인 해법들**:
    *   **전역적 순서**: 철학자가 항상 자신의 왼쪽 젓가락을 먼저 잡도록 강제하면 순환 의존성이 없어지고 데드락을 피할 수 있습니다.
    *   **중앙의 조정자 (웨이터)**: 중앙 조정자가 철학자에게 젓가락 두 개(한 쌍)를 주거나 아예 주지 않도록 합니다. 이렇게 하면 철학자가 1개의 젓가락만 갖는 상황 자체를 만들지 않으므로 점유와 대기 문제를 피하고 데드락이 발생하지 않습니다.
    *   **찬드라-미즈라 (Chandy-Misra)**: 젓가락을 '더러운' 상태와 '깨끗한' 상태로 표기하며, 철학자들은 서로 젓가락을 요청하는 메시지를 주고받습니다.
    *   **N-1 철학자**: N명의 철학자와 N개의 젓가락이 있는 테이블에서 정수형 세마포어를 통해 한 번에 최대 N-1명의 철학자만이 젓가락을 가질 수 있도록 제한합니다. 이렇게 하면 데드락과 라이브락을 해결할 수 있으며, 최악의 상황이라도 최소 한 명의 철학자는 성공적으로 2개의 젓가락을 가져갈 수 있게 됩니다. 하지만 추가로 '공정함' 개념을 도입하지 않으면 한 명의 철학자가 굶주릴(기아 상태) 수 있습니다.

### 4.8 병행성의 경험적 규칙 (Empirical Rules for Concurrency)

*   식사하는 철학자 문제는 거의 모든 병행 프로그램 문제에 적용할 수 있는 일반적인 원칙들과 방법론을 시사합니다.

#### 4.8.1 전역 순서 규칙 (Global Ordering Rule)

*   **문제**: 병행 프로그램에서는 사건의 발생이 단일 스레드 프로그램과는 달리 **프로그램 명령어 순서에 의해 결정되지 않습니다**.
*   **해결**: 만약 병행 시스템에서 순서를 강제해야 하는 경우, 이 순서는 **전역적으로 (globally)** 모든 스레드에 강제해야 합니다.
*   **이중 연결 리스트 예시**: 이중 연결 리스트의 디자인은 프로그램 순서가 데이터 순서와 동일하다는 가정이 깔려 있습니다. 하지만 다중 스레드 시스템에서는 데이터 경쟁이 발생하여 예상과 다른 결과가 나오거나 리스트가 훼손될 수 있습니다.
*   **적용**: 전역 순서 규칙을 통해서만 이 문제를 해결할 수 있습니다. 먼저 리스트의 전역 순서가 왜 보장돼야 하는지 질문하고, 어쩌면 순서 자체가 중요하지 않을 수도 있는지 고려해야 합니다. 만약 순서가 중요하지 않다면 단일 연결 리스트를 사용하고 항상 뒤에 원소를 추가하는 것이 쉬운 해결책입니다. 전역 순서가 반드시 필요하다면, 우연한 프로그램 순서에 영향을 받지 않는 안정적이고 결정론적인 정렬 규칙(예: 알파벳순, 우선순위 등)을 찾아내야 합니다. 이중 연결 리스트를 병행적으로 사용하는 것은 매우 어렵습니다.

#### 4.8.2 트랜잭션 기반 알고리듬 (Transaction-Based Algorithms)

*   **개념**: 식사하는 철학자 문제에서 중앙 조정자 해법과 유사하게, **나눌 수 없는 자원 또는 동작(둘 다)들의 묶음**을 트랜잭션이라고 합니다.
*   **원리**: 병행 시스템의 스레드는 중재자에게 트랜잭션 요청을 보냅니다. 트랜잭션은 **온전히 성공하거나 완전히 실패**합니다 (일부만 성공하고 일부만 실패하는 경우는 없습니다).
*   **실패 시**: 트랜잭션이 실패하면 스레드는 성공할 때까지 계속 트랜잭션 요청을 반복합니다 (중간에 잠시 대기할 수도 있습니다).
*   **구현**: 대부분의 작업을 **로컬로 처리**해야 합니다 (공유 데이터에 직접 작업하는 것이 아니라 현재 스레드만 볼 수 있는 데이터를 사용).
*   **커밋 (Commit)**: 모든 준비가 끝나고 트랜잭션을 커밋할 준비가 되면 하나의 **원자적 명령어** (예: CAS, LL/SC)를 실행합니다.
*   **효과**: 이 원자적 명령어가 성공하면 트랜잭션이 전체 시스템에 성공적으로 반영된 것이며 (공유 데이터에 영구적인 변화를 가함), 실패할 경우 다른 스레드가 동시에 트랜잭션을 커밋하고 있음을 의미합니다. 이 같은 실패 후 재시도 방식 덕분에 **최소한 시스템의 한 스레드는 언제나 일을 처리하게 됩니다** (락-프리(lock-free)의 정의).
*   **활용**: 트랜잭션 기반 알고리듬은 병행 시스템이나 분산 시스템에서 흔히 볼 수 있으며, 대부분의 락-프리 데이터 구조와 알고리듬의 기본 개념입니다.

#### 4.8.3 자원 공유 최소화 (Minimizing Resource Sharing)

*   **목표**: 병행 시스템이 가장 효율적으로 동작하는 것은 모든 스레드가 락을 기다릴 필요 없이 돌아갈 때입니다.
*   **방법**: **자원 공유를 최소화**하는 방법을 찾아보는 것입니다. 예를 들어, 각 스레드 그룹이 데이터를 생산한 후 자신만의 저장소에 저장하고, 모든 스레드가 결과물을 생산한 후 중심 스레드가 이를 합치는 방식입니다.
*   **적용**: 식사하는 철학자 문제에서 각 철학자에게 처음부터 젓가락 2개씩을 한 쌍으로 주면 병행성 자체를 문제에서 제거하는 셈이 됩니다 (공유 자원이 없으면 병행성도 없습니다). 실재하는 병행 시스템에서 공유 자원을 모두 제거할 수는 없지만, 락 사용을 최소화하기 위해 자원 공유를 최소화하는 방법을 모색할 수 있습니다.

#### 4.8.4 스레드 안정성 (Thread Safety) / 모니터 (Monitor)

*   **스레드 안정성 (Thread Safety)**: 보통 어떤 클래스나 함수형 API가 다중 스레드 프로세스의 어느 스레드에서 불리더라도 **올바르게 동작**하는 경우를 '스레드 안정성을 갖췄다'고 말합니다.
*   **획득 방법**: 함수의 제일 위에서 임계 구역에 진입하고, 작업을 수행한 후, 리턴하기 직전에 임계 구역을 빠져나오는 것이 가장 흔한 방법입니다.
*   **모니터 (Monitor)**: 클래스의 모든 함수가 스레드 안정성을 갖춘 경우 '모니터'라고 부르기도 합니다.
*   **문제점**: 클래스나 인터페이스가 스레드 안정성을 제공하면 편리하지만, 때로는 불필요한 부담을 야기하기도 합니다.
    *   **재진입 문제 (Reentrancy)**: 스레드 안전한 두 함수가 하나의 임계 구역을 사용하여 서로를 다시 호출하는 경우, 락을 다시 획득하지 못해 데드락에 빠질 수 있습니다. 이를 해결하려면 재진입 가능 락(reentrant lock)을 구현하거나, '불안정'한 함수를 '스레드 안정성'을 보장하는 래퍼 함수로 감싸는 방법이 있습니다.
*   **개념**: 100% 스레드 안정성을 갖춘 클래스와 API를 만들어 병행 프로그래밍을 처리하려는 시도는 좋지 않습니다. 불필요하게 비싼 인터페이스로 이어지기 쉽고, 프로그래머가 병행 환경에서 프로그래밍하고 있다는 사실을 인지하지 못하게 부추기는 경향이 있기 때문입니다.
*   **더 나은 전략**: 소프트웨어에 병행성이 존재한다는 사실을 인정하고 기꺼이 품어야 하며, 병행성을 명확하게 처리할 수 있는 자료 구조와 알고리듬을 디자인하도록 노력해야 합니다. 스레드 간 경쟁과 의존성을 줄이고, 락을 최소한으로 사용하는 소프트웨어 시스템을 만드는 것이 목표가 되어야 합니다. 완전한 락-프리 시스템을 구현하는 것은 매우 어렵지만, 락을 과용하여 '밀봉된' 인터페이스를 구현하려는 시도보다는 락 사용을 최소화하는 방향이 훨씬 나은 전략입니다.

### 4.9 락-프리 병행성 (Lock-Free Concurrency)

*   **개념**: 병행 시스템에서 경쟁 상태에 대한 해법으로 뮤텍스 락을 사용해 임계 구역을 원자적으로 만들고, 바쁜-대기를 피하기 위해 조건 변수와 스레드 수면 상태를 활용하는 방식이 있었습니다. **락-프리 병행성(lock-free concurrency)**은 스레드가 자원을 기다리는 동안 **수면 상태로 진입하는 것을 막는 방법**을 가리킵니다. 그래서 '블로킹-프리(blocking-free)'라고 불렀으면 더 알기 쉬웠을지도 모릅니다.
*   **논-블로킹 병행 프로그래밍 기법**: 락-프리 프로그래밍은 논-블로킹 병행 프로그래밍 기법들 중 한 부분에 불과합니다. 이 기법들은 스레드가 블록되면 진행을 멈춘다는 문제에 대응하여 **시스템 전체의 진행을 보장**하는 것을 목표로 합니다.
*   **진행 보장의 정도**:
    1.  **무 차단 (Obstruction Freedom)**: 한 스레드를 제외한 시스템 전체의 스레드가 대기 상태가 되더라도, 남은 한 스레드가 일정한 단계 안에 반드시 작업을 수행하도록 보장합니다.
        *   락-프리 알고리듬은 **트랜잭션 기반**입니다. 트랜잭션 수행이 다른 스레드에 의해 중단되면 트랜잭션을 되돌리고 (롤백), 성공할 때까지 다시 시도합니다.
        *   이 방식은 데드락을 피할 수 있지만 **기아 상태의 가능성**은 있습니다.
    2.  **무 잠금 (Lock Freedom)**: 모든 스레드의 진행을 보장하며, 어떤 스레드가 갑자기 정지되더라도 다른 스레드를 블로킹할 수 없습니다. 따라서 이 알고리듬에서는 뮤텍스와 같은 락을 사용할 수 없습니다.
    3.  **무 대기 (Wait Freedom)**: 무 잠금 알고리듬의 내용을 모두 보장하면서 **기아 상태에 빠지지 않도록** 보장합니다. 즉, 모든 스레드가 진행할 수 있으며, 특정 스레드가 무한히 기아 상태에 빠지지 않습니다.
*   **주제 범위**: 논-블로킹 알고리듬은 매우 폭넓은 주제이며, 여전히 연구가 진행 중인 분야입니다.

#### 4.9.1 데이터 경쟁의 원인 (Causes of Data Race)

*   데이터 경쟁은 임계 동작이 동일한 데이터에 대한 다른 임계 동작에 의해 중단될 때 발생합니다. 하지만 이 외에도 데이터 경쟁이 발생할 수 있는 요인이 두 가지 더 있습니다.
*   **세 가지 주요 원인**:
    1.  **실행 중단 (Interruption)**: 선점형 멀티태스킹에 의해서건 멀티코어 환경에서건 스레드는 항상 다른 스레드를 끊습니다. 데이터 경쟁 문제는 동일한 공유 데이터에 대한 임계 동작이 서로 끊어질 때 발생합니다.
    2.  **컴파일러나 CPU에 의한 명령어 재배열 (Instruction Reordering by Compiler or CPU)**:
        *   컴파일러는 파이프라인 정체를 최소화하려고 명령어를 재배열하기도 합니다. 마찬가지로 CPU의 비순차 실행 로직에 의해 프로그램 순서와 다른 순서로 명령어가 실행될 수 있습니다.
        *   명령어 재배열은 단일 스레드 프로그램의 관측 가능한 행동을 변경하지 않는 것을 보장하지만, **두 개 이상의 스레드가 공유 데이터에 동작하는 방식을 변경**할 수 있고, 따라서 **병행 프로그램의 데이터 경쟁을 일으킬 수 있습니다**.
    3.  **특정 하드웨어의 메모리 정렬 의미론 (Memory Ordering Semantics) 문제**:
        *   컴퓨터의 메모리 컨트롤러는 매우 공격적인 최적화를 하며, 이에 따라 특정 읽기/쓰기 명령어의 실제 실행이 시스템의 다른 읽기/쓰기에 비해 상대적으로 지연될 수 있습니다.
        *   컴파일러 최적화와 비순차 실행의 경우와 마찬가지로, 메모리 컨트롤러 최적화는 단일 스레드 프로그램의 표면적인 행동을 변경하지 않습니다. 하지만 병행 시스템에서 임계 동작의 읽기/쓰기 쌍의 순서를 변경할 가능성이 있고, 따라서 스레드가 데이터를 공유할 때 **예측 불가능한 행동 (메모리 정렬 버그)**을 일으킬 수 있습니다.
*   **원자성 보장**: 임계 동작에서 데이터 경쟁이 발생하지 않도록 하려면 이 세 가지 상황이 발생하지 않도록 해야 합니다.

#### 4.9.2 원자성 구현 (Implementing Atomicity)

#### 4.9.2.1 인터럽트를 끔으로써 원자성 획득하기 (Obtaining Atomicity by Disabling Interrupts)

*   다른 스레드가 현재 동작을 끊지 못하게 (인터럽트하지 못하게) 하기 위해, 동작 직전에 인터럽트를 끄고 동작이 끝난 후 다시 켤 수 있습니다.
*   **한계**: 이 방법은 선점형 멀티태스킹을 사용하는 **단일 코어 시스템에서만 가능**합니다. `cli` (clear interrupt enable bit)와 같은 명령어는 해당 명령어를 실행한 코어에만 영향을 미치며, 다른 코어들은 영향을 받지 않기 때문입니다.

#### 4.9.2.2 원자적 명령어 (Atomic Instructions)

*   '원자적'이라는 말은 어떤 동작을 더 이상 쪼갤 수 없는 상태까지 도달함을 암시합니다. 그렇다면 '원래' 원자적인 기계어 명령어들도 존재합니다. 즉, CPU에서 중단될 수 없는 (인터럽트할 수 없는) 명령어들이 있습니다.
*   일부 CPU는 어셈블리 코드에서 명령어에 **접두어(prefix)**를 붙임으로써 사실상 대부분의 명령어가 자동으로 원자적으로 실행되게 강제하기도 합니다 (예: x86 ISA의 `lock` 접두어).
*   이는 병행 프로그래밍을 하는 프로그래머에게 좋은 소식입니다. 이러한 원자적 명령어들 덕분에 뮤텍스와 같은 락 등, 더 큰 범주의 원자적 동작을 구현할 수 있게 됩니다.
*   CPU ISA마다 지원하는 원자적 명령어 집합과 규칙이 다릅니다. 크게 두 가지로 분류할 수 있습니다:
    *   **원자적 읽기 및 쓰기 명령어 (Atomic Read and Write Instructions)**.
    *   **원자적 읽기-변경-쓰기 (Atomic Read-Modify-Write, RMW) 명령어**.

#### 4.9.2.3 원자적 읽기-쓰기 (Atomic Reads and Writes)

*   대부분의 CPU는 4바이트 정렬된 32비트 정수에 대한 읽기나 쓰기가 **원자적일 것이라고 합리적으로 가정**할 수 있습니다 (CPU마다 다를 수 있으므로 ISA 문서 참조 필수).
*   이는 대부분의 CPU에서 그 크기가 레지스터보다 작거나 일치하는 메모리 정렬된 정수에 대한 읽기, 쓰기를 하나의 메모리 접근 주기 내에 처리할 수 있기 때문입니다. CPU가 이 동작을 수행할 때는 암묵적인 클럭과 동기화하므로 메모리 사이클은 다른 코어도 끊을 수 없습니다. 따라서 읽기와 쓰기 명령어는 원자성을 띠는 셈이 됩니다.
*   **메모리 정렬되지 않은 읽기/쓰기**: 통상적으로 원자성을 보장받지 못합니다. 메모리 정렬이 안 된 데이터를 읽거나 쓰려면 CPU가 두 개 이상의 메모리 접근을 시도해야 하기 때문에 명령어 수행이 중단될 수 있으며 원자성을 보장할 수 없습니다.

#### 4.9.2.4 원자적 읽기-변경-쓰기 (Atomic Read-Modify-Write, RMW)

*   원자적 읽기나 원자적 쓰기만으로는 일반적 의미의 원자적 동작을 구현하기에 부족합니다. 뮤텍스와 같은 락을 구현하려면 메모리로부터 변수의 내용을 읽고, 그 값에 모종의 동작을 수행한 후, 그 결과를 메모리에 다시 저장하는 과정이 **중단받지 않을(인터럽트되지 않을) 방법**이 필요합니다.
*   **현대 CPU 지원**: 현대의 대부분 CPU는 병행 프로그래밍을 위해 적어도 하나 이상의 **원자적 읽기-변경-쓰기 명령**을 지원합니다. 이 명령어들은 뮤텍스나 스핀 락을 구현하는 데 사용될 수 있습니다.

#### 4.9.2.5 테스트-앤드-세트 (Test-and-Set, TAS)

*   **정의**: 가장 단순한 RMW 명령어로, 실제로 값을 검사하고 변경하는 동작은 아닙니다. 실제로는 불리언 변수를 **원자적으로 `true`로 변경**한 다음 **이전 값을 리턴**합니다 (따라서 이 값을 테스트할 수 있습니다).
*   **활용**: 가장 단순한 형태의 락인 **스핀 락(Spin Lock)**을 구현하는 데 사용할 수 있습니다.
*   **`PAUSE()` 매크로**: 바쁜-대기(busy-waiting) 루프 내에서 전력 소모를 줄이거나 CPU 파이프라인의 흐름을 비우는 데 사용됩니다.

#### 4.9.2.6 교환 (Exchange)

*   Intel x86 등 일부 ISA는 원자적 **교환(exchange)** 명령을 제공합니다. 이 명령어는 두 레지스터 간 또는 레지스터와 메모리 위치 간의 내용을 교체합니다. x86에서는 레지스터와 메모리의 값을 교환할 때 **기본적으로 원자적으로 동작**합니다 (명령어가 `lock` 접두어를 가진 것처럼 행동).

#### 4.9.2.7 비교 및 교체 (Compare-and-Swap, CAS)

*   **정의**: 일부 CPU는 **비교 및 교체(compare-and-swap, CAS)** 명령어를 제공합니다. 이 명령어는 메모리 특정 위치에 존재하는 값을 **읽은 후**, 프로그래머가 제공한 값과 이 메모리 값이 **일치하는 경우에만** 새 값으로 교체합니다. 동작이 성공한 경우에는 `true`를 리턴하며, 실패하면 `false`를 리턴합니다.
*   **다양한 값 지원**: CAS는 불리언보다 큰 값에 대해 동작할 수 있으며, 최소한 32 또는 64비트 정수를 위한 함수가 보통 제공됩니다.
*   **RMW 구현**: CAS를 통해 읽기-변경-쓰기 동작을 구현하려면 다음 접근법을 따릅니다:
    1.  변경하려는 변수의 이전 값을 읽습니다.
    2.  원하는 대로 이 값을 변경합니다.
    3.  결과를 다시 쓸 때는 일반적인 쓰기 대신 CAS 명령어를 사용합니다.
    4.  CAS가 성공할 때까지 반복합니다.
*   **데이터 경쟁 감지**: CAS 명령어를 통해 데이터 경쟁이 있는지 알 수 있습니다. 읽기-변경-쓰기 동작 전의 값과 쓰기 시점의 메모리 값을 비교하여 데이터 경쟁이 없는 경우 CAS는 단순한 쓰기 명령과 동일하게 작동합니다. 그러나 읽기와 쓰기 간에 메모리의 값이 변한 경우 다른 스레드가 선수를 쳤음을 알 수 있고, 이때는 다시 시도해야 합니다.

#### 4.9.2.8 ABA 문제 (ABA Problem)

*   **정의**: CAS 명령어가 **한 가지 종류의 데이터 경쟁을 탐지할 수 없는 문제**를 'ABA 문제'라고 합니다.
*   **시나리오**: 스레드 A가 값을 'A'로 읽었다고 가정합니다. 이 상태에서 CAS 명령어를 실행하기 전에 스레드 A가 다른 스레드에 의해 선점당하고, 이 다른 스레드가 원자적으로 갱신하려는 메모리 주소의 값을 두 번 갱신합니다. 첫 번째는 'B' 값으로 갱신하고, 두 번째는 다시 'A' 값으로 갱신합니다. 이제 스레드 A의 CAS 명령어가 실행할 상황이 되었을 때, 이전에 읽었던 값('A')과 현재 메모리 값('A')이 동일하므로 데이터 경쟁이 발생하지 않았을 것이라고 **잘못 판단**하게 됩니다.

#### 4.9.2.9 로드 링크드/스토어 컨디셔널 (Load-Linked/Store-Conditional, LL/SC)

*   **개념**: 일부 CPU는 비교 및 교체 동작을 **로드 링크드(load-linked, LL)** 명령어와 **스토어 컨디셔널(store-conditional, SC)** 명령어 두 개로 나눕니다.
    *   **LL 명령어**: 메모리 주소의 값을 원자적으로 읽어 오고, 동시에 특수한 CPU 레지스터인 **링크 레지스터(link register)**에 해당 주소를 저장합니다.
    *   **SC 명령어**: 지정한 메모리 주소에 값을 쓰는데, 이때 주소가 **링크 레지스터의 값과 일치할 때만** 동작을 수행합니다. 쓰기 동작이 성공하면 `true`를 리턴하고, 그렇지 않으면 `false`를 리턴합니다.
*   **ABA 문제 해결**: 메모리를 통한 모든 쓰기 동작(SC 명령 포함)은 링크 레지스터의 값을 0으로 초기화합니다. 이는 LL/SC 명령어 짝이 데이터 경쟁을 감지할 수 있음을 의미합니다. 왜냐하면 LL과 SC 사이에 어떠한 쓰기 동작이라도 발생할 경우 SC는 실패하기 때문입니다.
*   **RMW 구현**: LL/SC 명령어를 일상적인 읽기-변경-쓰기 동작과 같이 사용할 수 있습니다.

#### 4.9.2.10 CAS에 대한 LL/SC의 상대적 장점 (Advantages of LL/SC over CAS)

*   LL/SC 명령어 짝은 CAS 명령어에 비해 두 가지 명확한 장점이 있습니다.
    1.  **ABA 문제 없음**: SC 명령은 쓰기 명령이 한 번이라도 수행된 경우 실패하기 때문에 LL/SC는 ABA 문제가 없습니다.
    2.  **파이프라인 친화적**: CAS 명령은 두 번의 메모리 접근 주기(기대 값 비교 및 값 쓰기)가 필요한 반면, LL과 SC 명령은 각각 한 번의 메모리 접근 주기만 필요하므로 파이프라인 구조에 매우 잘 들어맞습니다.

#### 4.9.2.11 엄격한 비교 교환, 느슨한 비교 교환 (Strong Compare-Exchange, Weak Compare-Exchange)

*   **C++11 표준 라이브러리**: 이식 가능한 원자적 비교-교환 동작을 제공하며, 하드웨어에 따라 CAS 또는 LL/SC로 구현될 수 있습니다.
*   **두 가지 버전**: SC 동작은 매우 빈번하게 실패하기 때문에 C++11은 **엄격한(strong)** 버전과 **느슨한(weak)** 버전 두 가지 비교-교환 연산을 제공합니다. 엄격한 비교-교환은 빈번한 SC 실패를 프로그래머로부터 숨기고, 느슨한 비교-교환은 그렇지 않습니다.

#### 4.9.2.12 원자적 RMW 명령어들의 상대적 효용성 (Relative Usefulness of Atomic RMW Instructions)

*   TAS(Test-and-Set) 명령은 불리언 값만 다루기 때문에 무-대기 합의 문제(wait-free consensus problem)를 두 스레드에 대해서만 해결할 수 있습니다.
*   CAS(Compare-and-Swap)는 32비트 값을 다루기 때문에 스레드 개수에 상관없이 이 문제를 해결할 수 있습니다.

#### 4.9.3 장벽 (Barrier)

*   데이터 경쟁 버그의 원인이 실행 중단(인터럽트)만 있는 것은 아닙니다. 컴파일러와 CPU가 수행하는 **명령어 재배열(instruction reordering)** 최적화는 병행 프로그램에서 미묘한 버그를 유발할 수 있습니다.
*   **문제 발생 원리**: 컴파일러나 CPU의 비순차 실행 로직은 시스템에서 어떤 다른 스레드가 실행 중인지, 무엇을 하는지에 대해서는 전혀 알지 못합니다. 따라서 단일 스레드 환경에서는 가시적인 동작 변화가 없도록 보장하지만, **병행 프로그램에서는 명령어 재배열이 문제를 일으킬 수 있습니다**.
*   **생산자-소비자 예시**: `g_data = 42;`와 `g_ready = 1;` 두 줄의 순서가 컴파일러나 CPU에 의해 바뀔 수 있어, 소비자가 `g_ready`가 1인 것을 보고 `g_data`를 읽었을 때 아직 42가 아닐 수 있는 문제가 발생합니다.
*   **C/C++의 `volatile` 키워드**:
    *   `volatile`은 메모리 맵 I/O나 시그널 핸들러 동작을 보장하기 위한 타입 수식어입니다. 이는 변수의 내용이 레지스터에 캐시되지 않음을 보장할 뿐입니다.
    *   일부 컴파일러는 `volatile` 변수의 읽기/쓰기 사이에 명령어 재배열을 하지 않는다는 보장을 하기도 하지만 이는 일부이며, 이식 가능한 코드에서는 의존할 수 없습니다.
    *   결정적으로 `volatile` 키워드는 **CPU의 비순차 실행 로직에 의한 명령어 재배열에는 전혀 영향을 줄 수 없습니다**. 따라서 C/C++ 코드에서 안정적인 병행 소프트웨어를 짜는 데 `volatile`은 별 도움이 되지 않습니다. (단, 자바나 C# 등 일부 언어에서는 `volatile` 키워드가 원자성을 보장합니다).

#### 4.9.3.3 컴파일러 장벽 (Compiler Barrier)

*   컴파일러가 임계 동작 범위 안의 읽기/쓰기를 재배열하지 못하게 하려면 **컴파일러 장벽(compiler barrier)**이라는 특수한 의사 명령어를 코드에 삽입하여 직접 지시할 수 있습니다.
*   **예시**: GCC에서는 인라인 어셈블리 문법 `asm volatile("" ::: "memory")`을 사용하여 장벽을 칩니다. Visual C++에서는 `_ReadWriteBarrier()`가 이 역할을 합니다.
*   **한계**: 컴파일러 장벽은 **CPU의 비순차 실행 로직이 런타임에 명령어를 재배열하는 것은 막지 못합니다**.

#### 4.9.4 메모리 정렬 의미론 (Memory Ordering Semantics)

*   **문제**: 컴파일러와 CPU에 의한 기계어 재배열과 별개로, 병렬 시스템에서는 **읽기/쓰기 명령어가 사실상 재배열될 가능성**이 존재합니다. 특히 멀티레벨 캐시가 탑재된 멀티코어 시스템의 경우, 읽기/쓰기 명령어의 실행 순서에 대해 여러 코어에서 서로 다른 판단을 내릴 수 있습니다.
*   **메모리 정렬의미론 (Memory Ordering Semantics)**: 모든 CPU는 메모리 정렬 의미론이라는 엄격한 규칙에 따라 동작합니다. 이 규칙은 코어 사이에 읽기와 쓰기 명령어가 어떤 식으로 전달되는지에 대한 여러 가지 보장을 지원하며, 기본적인 보장으로 부족한 경우 프로그래머가 특정 정렬을 강제할 수 있는 도구를 제공합니다.
*   **CPU별 차이**: 강한 보장을 지원하는 CPU(예: Intel x86)와 약한 보장을 지원하는 CPU(예: DEC Alpha, PowerPC)가 있습니다. 약한 보장 CPU에서는 메모리 정렬 문제를 해결하기 위해 더 많은 펜스가 필요합니다.

#### 4.9.4.1 메모리 캐시 동작 방식 (How Memory Caches Work)

*   CPU는 느린 주메모리 접근 시간을 피하기 위해 자주 사용되는 데이터를 캐시(L1, L2 등)에 보관합니다. CPU가 데이터에 접근할 때 데이터가 캐시에 존재한다면 항상 캐시의 복사본에 접근합니다.
*   데이터가 업데이트되면 먼저 L1 캐시에 반영되고, 효율성을 위해 주메모리에는 즉시 라이트백(write-back)하지 않고 보류되는 경우가 많습니다.

#### 4.9.4.2 멀티코어 캐시 일관성 프로토콜 (Multi-Core Cache Coherency Protocol)

*   멀티코어 머신에서 코어들 간에 로컬 L1 캐시의 데이터를 공유하는 통신 방법이 **캐시 일관성 프로토콜(cache coherency protocol)**입니다.
*   대부분의 CPU는 **MESI 프로토콜 (Modified, Exclusive, Shared, Invalid)**을 사용합니다.
    *   **Modified (수정)**: 캐시 라인이 수정됨 (dirty).
    *   **Exclusive (독점)**: 해당 주메모리 블록이 이 코어의 L1 캐시에만 있으며, 다른 코어는 복사본 없음.
    *   **Shared (공유)**: 주메모리 블록이 2개 이상의 코어 L1 캐시에 있으며, 모두 동일한 복사본 가짐.
    *   **Invalid (무효)**: 캐시 라인 데이터가 더 이상 유효하지 않음.
*   **동작 예시 (생산자-소비자)**:
    *   생산자(코어 1)가 `g_ready`를 쓰면, 코어 1의 L1 캐시가 업데이트되고 캐시 라인은 **수정(Modified)** 상태가 됩니다. 동시에 무효화(Invalidate) 메시지가 인터커넥트 버스(ICB)를 통해 전파되어 코어 2의 `g_ready` 복사본은 **무효(Invalid)** 상태로 변경됩니다.
    *   소비자(코어 2)가 `g_ready`를 읽으려 할 때 자신의 캐시 라인이 무효 상태임을 인지하고, ICB를 통해 Read 메시지를 전파하여 코어 1의 L1 캐시에서 새 값을 전달받습니다. 이 과정에서 두 코어의 캐시 라인은 다시 **공유(Shared)** 상태로 변경되고, 주메모리에도 라이트백이 발생합니다.

#### 4.9.4.4 MESI의 최적화 (Optimizations to MESI)

*   MESI 프로토콜은 지연 시간을 최소화하기 위해 강도 높게 최적화됩니다. 즉, ICB를 통해 받은 메시지를 즉시 처리하지 않고 미루는 경우가 있습니다.
*   **문제 발생**: 이러한 최적화로 인해 특정 상황에서는 두 개의 읽기/쓰기 동작이 **다른 코어의 관점에서 실제로 수행된 순서와 반대로 보이게 만들 수 있습니다**. 예를 들어, 생산자(코어 1)가 `g_data`를 쓴 후 `g_ready`를 변경할 때, `g_ready`의 새로운 값이 `g_data`의 새로운 값보다 캐시 일관성 도메인 내의 다른 코어(소비자)에 **먼저 알려질 수 있습니다**. 이는 소비자가 `g_data`의 올바른 값을 보기 전에 `g_ready`가 1인 것을 먼저 보게 되는 데이터 경쟁 문제입니다.

#### 4.9.4.5 메모리 펜스 (Memory Fence)

*   **개념**: 캐시 일관성 프로토콜 최적화 때문에 명령어들의 표면적 순서가 뒤바뀌는 현상을 막기 위해 현대의 CPU는 **메모리 펜스(memory fence)** 또는 **메모리 장벽(memory barrier)**이라는 특수한 기계어 명령을 제공합니다.
*   **지나치는 경우**: 읽기가 읽기를 지나치는 경우, 읽기가 쓰기를 지나치는 경우, 쓰기가 쓰기를 지나치는 경우, 쓰기가 읽기를 지나치는 경우 총 네 가지의 메모리 순서 변경을 막을 수 있습니다.
*   **풀 펜스 (Full Fence)**: 가장 강력한 형태의 펜스로, 프로그램 순서상 자신보다 앞선 모든 메모리 동작의 효과가 먼저 발생하고, 자신보다 뒤에 있는 모든 메모리 동작의 효과가 나중에 발생하도록 강제합니다.
*   **부가 효과**: 메모리 펜스는 CPU 캐시 일관성 프로토콜로 인한 메모리 순서 문제를 방지할 뿐만 아니라, **CPU의 비순차 실행 로직에 의한 명령어 재배열도 방지**합니다. 따라서 원자적 명령어와 메모리 펜스만 있으면 신뢰할 수 있는 뮤텍스, 스핀 락, 또는 락-프리 알고리듬을 구현할 수 있습니다.

#### 4.9.4.6 어과이어 의미론과 릴리스 의미론 (Acquire Semantics and Release Semantics)

*   **메모리 정렬 의미론 (Memory Ordering Semantics)**: 펜스의 역할은 읽기/쓰기 명령어가 특정한 메모리 정렬 의미론을 갖도록 프로그래머가 강제할 수 있는 도구를 제공하는 것입니다.
*   **세 가지 의미론**:
    1.  **릴리스 의미론 (Release Semantics)**: 공유 메모리에 대한 쓰기(쓰기-릴리스)는 프로그램 순서상 그보다 선행하는 다른 읽기나 쓰기가 지나칠 수 없습니다. 즉, 이전에 수행된 모든 메모리 동작이 메모리에 반영된 후 쓰기-릴리스가 발생하도록 보장합니다.
    2.  **어과이어 의미론 (Acquire Semantics)**: 공유 메모리에 대한 읽기(읽기-어과이어)는 프로그램 순서상 그보다 나중에 오는 다른 읽기/쓰기가 지나칠 수 없습니다. 즉, 어과이어 읽기 이후에 수행된 모든 메모리 동작이 이 읽기보다 나중에 발생하도록 강제하며, 다른 코어에서 수행한 모든 쓰기가 메모리에 반영된 후 읽기-어과이어가 발생하도록 보장합니다.
    3.  **풀 펜스 의미론 (Full Fence Semantics)**: 양방향 의미론으로, 코드의 펜스 명령어를 경계로 모든 메모리 동작들이 프로그램 순서대로 효과를 발생하도록 강제합니다.
*   **실존하는 CPU**: 각 CPU가 지원하는 개별 펜스 명령어들은 위 세 가지 의미론 중 적어도 한 가지를 지원합니다.

#### 4.9.4.7 어과이어 의미론과 릴리스 의미론을 언제 사용할 것인가 (When to Use Acquire and Release Semantics)

*   **쓰기-릴리스 (Write-Release)**: 주로 **생산자 시나리오**에서 사용됩니다. 한 스레드가 두 개의 연속된 쓰기(예: `g_data`를 쓴 후 `g_ready`를 쓰는 경우)를 올바른 순서로 인지하게 강제할 때, 두 번째 쓰기를 쓰기-릴리스로 만듭니다. 이는 릴리스 의미론을 제공하는 펜스 명령어를 쓰기-릴리스 명령어 앞에 배치하여 구현합니다.
*   **읽기-어과이어 (Read-Acquire)**: 보통 **소비자 시나리오**에서 사용됩니다. 한 스레드가 두 개의 연속된 읽기를 하며, 첫 번째 읽기가 특정 조건(예: `g_ready`가 `true`인 경우)에 대한 읽기일 때 이 순서를 강제해야 합니다. 이는 어과이어 의미론을 지원하는 펜스 명령어를 읽기-어과이어 명령어 뒤에 배치하여 구현합니다.
*   **생산자-소비자 락-프리 예시**:
    *   **생산자**: `g_data = 42;` 다음에 `RELEASE_FENCE();`를 배치하여 `g_ready = 1;` 쓰기가 `g_data` 쓰기 이후에 메모리에 반영되도록 합니다.
    *   **소비자**: `while (!g_ready) PAUSE();` 다음에 `ACQUIRE_FENCE();`를 배치하여 `g_data`를 읽기 전에 `g_ready`가 1이 된 것이 확실히 메모리에 반영되도록 합니다.

#### 4.9.4.8 CPU 메모리 모델의 종류 (Types of CPU Memory Models)

*   CPU마다 강한 메모리 정렬 의미론을 제공하는 경우(예: Intel x86)와 약한 메모리 정렬 의미론을 지원하는 경우(예: DEC Alpha, PowerPC)가 있습니다.
*   약한 의미론을 지원하는 CPU는 더 많은 메모리 펜스가 필요하지만, 강한 의미론을 지원하는 CPU는 기본적으로 더 적은 펜스로도 올바르게 동작할 수 있습니다.

#### 4.9.4.9 실존하는 CPU에서의 펜스 명령어들 (Fence Instructions on Real-World CPUs)

*   **Intel x86 ISA**: `sfence` (릴리스), `lfence` (어과이어), `mfence` (풀 펜스)를 구현합니다. `lock` 접두어는 원자적 동작과 함께 메모리 펜스 효과를 제공합니다.
*   **PowerPC ISA**: 상당히 약한 메모리 정렬을 사용하므로 명시적인 펜스 명령어가 보통 필요합니다. `sync` (풀 펜스), `lwsync` (경량), `eieio` (I/O) 등의 명령어를 지원합니다.
*   **ARM ISA**: 명령어 재배열 장벽 역할만 하는 `isb`, 두 개의 풀 펜스 명령인 `dmb`와 `dsb`, 단방향 읽기-어과이어 명령인 `ldar`, 단방향 쓰기-릴리스 명령인 `stlr`을 지원합니다.

#### 4.9.5 원자적 변수들 (Atomic Variables)

*   **문제점**: 원자적 명령어와 메모리 펜스를 직접 사용하는 것은 번거롭고 실수할 가능성이 크며 이식성이 낮습니다.
*   **C++11 `std::atomic<T>`**: C++11의 `std::atomic<T>` 템플릿 타입은 사실상 모든 데이터 타입을 감싸서 원자성을 보장합니다 (`std::atomic_flag`는 불리언 타입에 특수화).
*   **기본 의미론**: `std::atomic` 계열 템플릿 구현은 기본적으로 변수에 **어과이어/릴리스(acquire/release) 메모리 정렬 의미론**을 부여합니다 (더 약한 의미론도 지정 가능).
*   **결과**: 데이터 경쟁 버그를 유발하는 세 가지 요인으로부터 자유로운 **락-프리 코드**를 작성할 수 있게 됩니다.
*   **생산자-소비자 예시**: `std::atomic<float> g_data;`와 `std::atomic<bool> g_ready = false;`를 사용하여, 이전에 경쟁 상태를 일으켰던 코드를 안전하게 만듭니다.
*   **구현 복잡성**: `std::atomic<T>`의 실제 구현은 복잡합니다. C++ 표준 라이브러리는 이식성을 보장해야 하므로 대상 플랫폼에서 사용할 수 있는 원자적 기능과 장벽을 지원하는 기계어 명령어를 사용하여 구현됩니다.
*   **`is_lock_free()`**: `std::atomic` 변수에 대해 `is_lock_free()` 함수를 호출하여 해당 플랫폼에서 락 없이 구현되었는지 확인할 수 있습니다.

#### 4.9.5.1 C++ 메모리 정렬 (C++ Memory Ordering)

*   **기본값**: C++ 원자적 변수들은 기본적으로 **풀 펜스 장벽**을 사용하기 때문에 대부분의 경우 올바르게 작동합니다.
*   **보장 정도 조절**: `std::memory_order` 타입의 부가적인 함수 인자를 넘김으로써 원자적 동작의 메모리 정렬 보장 정도를 낮출 수 있습니다.
    1.  `std::memory_order_relaxed`: 이 설정으로 실행된 원자적 동작은 오직 **원자성만 보장**하며, 장벽이나 펜스는 사용되지 않습니다.
    2.  `std::memory_order_consume`: 이 의미론을 갖고 실행된 읽기 동작에 대해, 같은 스레드의 다른 읽기/쓰기 동작이 이 읽기 동작보다 **먼저 오도록 재정렬될 수 없습니다**. 즉, 컴파일러 최적화와 비순차 실행에 의한 명령어 재배열을 막는 용도로만 쓰입니다.
    3.  `std::memory_order_release`: 이 의미론으로 실행된 쓰기에 대해, 동일 스레드의 다른 읽기/쓰기가 이 쓰기 동작보다 **뒤에 오도록 재배열될 수 없으며**, 동일한 주소를 읽는 다른 스레드들이 해당 쓰기 결과를 볼 수 있도록 보장합니다.
    4.  `std::memory_order_acquire`: 컨숨 의미론을 보장하는 동시에, 다른 스레드가 같은 주소에 쓸 경우 이 스레드에게 결과가 보여짐을 보장합니다.
    5.  `std::memory_order_acq_rel`: 읽기와 쓰기 모두에 풀 펜스 의미론을 적용하며, **기본값이고 가장 안전**합니다.
*   **플랫폼 의존성**: 메모리 정렬 지정자를 사용하더라도 모든 플랫폼에서 지정된 의미론이 반드시 사용되는 것은 아닙니다. 최소한의 의미론만을 보장하며, 하드웨어에 따라 더 강한 의미론을 사용할 수 있습니다. 예를 들어, Intel x86에서는 느슨한 정도의 메모리 정렬이 불가능한데, 이는 CPU의 기본 메모리 정렬 의미론이 상대적으로 더 강하기 때문입니다.
*   **사용법**: `std::atomic`의 오버로드된 할당 연산자와 형 변환 연산자 대신 `store()`, `load()`를 직접 사용해야 합니다.
*   **주의**: 메모리 정렬 지정자를 사용할 때는 **80/20 규칙**을 명심해야 합니다. 성능 향상이 반드시 필요한 경우(프로파일링으로 증명)에만 사용하고, 의도한 효과가 있다는 것을 증명해야 합니다.

#### 4.9.6 인터프리트 프로그래밍 언어에서의 병행성 (Concurrency in Interpreted Programming Languages)

*   C, C++과 같은 컴파일 언어는 최종적으로 CPU에서 직접 실행되는 기계어로 변환됩니다. 따라서 원자적 동작과 락을 구현하려면 원자적 동작과 캐시 일관적 메모리 장벽을 제공하는 특수한 기계어 명령과 커널의 도움(스레드 수면/깨움)이 필요합니다.
*   **자바(Java)나 C#과 같은 인터프리트 언어**는 사정이 다릅니다. 이 언어들의 프로그램은 **가상 머신(Virtual Machine, VM)**의 문맥에서 실행됩니다. VM은 CPU를 소프트웨어적으로 흉내 낸 것으로, 바이트코드 명령어들을 순서대로 읽어 실행합니다. VM은 운영체제 커널과 같은 역할도 수행하며, 고유한 '스레드' 개념(바이트코드 명령어로 이루어짐)을 지원하고 스레드를 스케줄링하는 데 필요한 모든 것을 직접 처리합니다.
*   **장점**: VM이 완전히 소프트웨어로 구현되기 때문에, 자바나 C# 등의 인터프리트 언어는 C/C++ 등의 컴파일 언어에 비해 하드웨어의 제약을 덜 받는 **훨씬 강력한 병행성 동기화 기능**을 제공할 수 있습니다.
*   **`volatile` 타입 한정자**: C/C++에서 `volatile` 변수는 원자적 변수가 아니지만, 자바와 C#에서는 `volatile` 타입 한정자가 **진짜 원자성을 보장**합니다.
    *   이 언어들에서 `volatile` 변수에 대한 동작은 최적화 대상이 아니며, 다른 스레드에 의해 중단될 수 없습니다.
    *   뿐만 아니라 `volatile` 변수에 대한 읽기는 캐시가 아닌 주메모리에서 직접 이루어지며, 쓰기 또한 캐시가 아닌 주메모리에 쓰이게 됩니다.
    *   이러한 보장을 제공할 수 있는 이유 중 하나는 **가상 머신이 자신의 바이트코드 명령어들을 어떻게 실행할지에 대한 완전한 제어권**을 행사하기 때문입니다.

#### 4.9.7 스핀 락 (Spin Lock)

*   **개념**: 스핀 락은 `std::atomic_flag`와 TAS(Test-and-Set) 명령어를 활용하여 구현할 수 있습니다.
*   **획득 (Acquire)**: 락을 획득하는 것은 TAS 명령을 통해 원자적으로 플래그에 `true`를 쓰는 행위이며, TAS가 성공할 때까지 `while` 루프에서 **바쁜-대기**를 반복해야 합니다.
*   **해제 (Release)**: 락을 해제하는 것은 원자적으로 플래그에 `false`를 쓰는 행위입니다.
*   **메모리 정렬 의미론**:
    *   **획득**: `Acquire()`에서 `acquire` 메모리 정렬 의미론을 사용해야 합니다 (`std::atomic_flag::test_and_set`에 `std::memory_order_acquire`). 이는 다른 스레드가 임계 구역을 완전히 빠져나오지 않았는데도 락이 해제된 것처럼 관측되는 상황을 방지합니다.
    *   **해제**: `Release()`에서 `release` 메모리 정렬 의미론을 사용해야 합니다 (`std::atomic_flag::clear`에 `std::memory_order_release`). 이는 락을 해제하기 전에 모든 쓰기 동작이 메모리에 반영되도록 보장합니다.

#### 4.9.7.2 범위 락 (Scoped Lock)

*   **필요성**: 뮤텍스나 스핀 락을 일일이 수동으로 해제하는 것은 불편하고 실수하기 쉽습니다 (특히 함수 내 리턴 지점이 여러 개일 경우).
*   **개념**: C++에서는 **범위 락(scoped lock)**이라는 간단한 래퍼 클래스(wrapper class)를 사용하여 특정 범위를 벗어날 경우 락이 자동으로 해제되게 할 수 있습니다.
*   **구현**: 생성자에서 락을 획득하고, 소멸자에서 락을 해제하기만 하면 됩니다.
*   **활용**: 이 범위 락 클래스는 `Acquire()`와 `Release()`를 구현하는 모든 락 클래스(스핀 락, 뮤텍스 등)와 함께 사용할 수 있습니다.

#### 4.9.7.3 재 진입 가능 락 (Reentrant Lock)

*   **문제점**: 기본 기능만 구현된 스핀 락은 자신이 이미 가진 락을 다시 획득하려 할 경우 스레드를 데드락에 빠뜨립니다. 이 상황은 두 개 이상의 스레드 안전한 함수들이 하나의 락 내에서 서로를 다시 호출하는 경우에 발생합니다.
*   **해결**: 락 클래스가 락을 가진 스레드의 ID를 기억하는 방법이 있습니다. 이렇게 하면 이미 락을 가진 스레드가 다시 락을 획득하려 하는 상황인지 (이것은 허용), 아니면 한 스레드가 가진 락을 다른 스레드가 획득하려는 상황인지 (이것은 기다리도록 함)를 판단할 수 있게 됩니다.
*   **참조 카운트**: `Acquire()`와 `Release()` 함수에 대한 호출이 짝이 맞도록 보장하기 위해 클래스에 **참조 카운트(reference count)**도 추가해야 합니다.
*   **구현 예시**: `ReentrantLock32` 클래스 (CAS를 사용하여 스레드 ID와 참조 카운트를 관리하며, 적절한 메모리 펜스를 사용).

#### 4.9.7.4 복수 읽기 - 단일 쓰기 락 (Multiple-Readers, Single-Writer Lock, MRSW Lock)

*   **개념**: 여러 스레드가 정해진 공유 데이터에 동시에 읽기/쓰기를 할 수 있을 때 뮤텍스나 스핀 락을 사용하여 데이터 접근을 통제할 수 있습니다. 하지만 **읽기는 동시에 여러 스레드가 할 수 있어야 하고**, 공유 데이터에 변화가 생길 경우에만 배타적 접근을 보장하면 되는 상황에 사용됩니다.
*   **특징**: 여러 읽는 이(readers)가 락을 동시에 획득할 수 있습니다. 쓰는 이(writers)가 락을 획득하려 시도하는 경우, 먼저 모든 읽는 이가 읽기 동작을 완료할 때까지 기다린 후, 특수한 '배타적' 모드로 락을 획득합니다. 이 상태에서는 쓰는 이가 데이터를 다 쓸 때까지 다른 읽는 이와 쓰는 이가 락을 획득할 수 없습니다.
*   **다른 이름**: `Shared-Exclusive Lock`, `Read-Write Lock` 또는 `Push Lock`이라고도 합니다.
*   **구현**: 읽는 이의 수를 나타내는 **참조 카운트(reference count)**를 기록합니다. 쓰는 이를 위한 '배타적' 락은 특정 큰 숫자(예: `0xFFFFFFFFU` 또는 가장 높은 비트 예약)로 표현하여 읽는 이의 카운트와 구분합니다.
*   **문제점**: MRSW 락은 **기아 상태(starvation)**를 야기할 수 있습니다. 쓰는 이가 락을 너무 오래 잡고 있으면 읽는 이들이 기아 상태가 되고, 반대로 읽는 이가 너무 많을 경우 쓰는 이가 기아 상태가 될 수 있습니다.
*   **해결**: **순차 락(seqlock)**은 기아 상태를 해결하는 한 방법일 수 있습니다.
*   **RCU (Read-Copy-Update)**: 리눅스 커널에서 사용하는 흥미로운 락 기법으로, 다수의 읽는 이와 다수의 쓰는 이를 동시에 지원합니다.

#### 4.9.7.5 락-필요 없음 어서션 (Lock-Not-Needed Assertion)

*   **목적**: 락은 항상 비용이 발생하며, 때로는 프로그래머가 **락이 필요 없는 상황임을 경험적으로 아는 경우**가 있습니다 (예: 게임 루프 구조상 두 스레드가 절대로 겹치지 않는 경우). 이런 상황에서 락을 사용하지 않으면서도, 만약 조건이 깨졌을 때 이를 감지할 수 있는 방법입니다.
*   **방법**: **`assert`를 통해 락이 필요하지 않음을 강제**하는 것입니다.
*   **장점**:
    *   **비용이 매우 저렴**합니다 (릴리즈 코드에서는 어서트를 아예 뺄 수도 있음).
    *   스레드의 겹침에 대해 가정했던 조건이 옳지 않다면 **자동으로 문제를 감지**할 수 있습니다. 나중에 리팩토링 때문에 조건 자체가 깨지는 것도 알 수 있습니다.
*   **구현**: 원자적 불리언 변수 대신 **`volatile bool` 변수**를 사용합니다. `volatile` 키워드는 병행성 경쟁 문제를 해소하는 데 직접적인 도움이 안 되지만, 불리언 변수에 대한 읽기와 쓰기가 최적화로 사라지는 것을 막아줍니다. 이렇게 함으로써 적당한 수준의 감지율을 제공하며 엄청나게 저렴합니다.
*   **`UnnecessaryLock` 클래스**: `Acquire()`에서 `assert(!m_locked); m_locked = true;`를, `Release()`에서 `assert(m_locked); m_locked = false;`를 실행합니다.
*   **활용**: `ASSERT_LOCK_NOT_NECESSARY` 매크로와 같은 범위 락 형태의 래퍼를 사용하여 특정 코드 블록의 시작과 끝에 어서션을 삽입할 수 있습니다.

#### 4.9.8 락-프리 트랜잭션 (Lock-Free Transactions)

*   **목적**: 락-프리 프로그래밍의 목적은 당연히 **락을 피하는 것**입니다. 락은 스레드를 수면 상태로 만들거나 스핀 락 내의 바쁜-대기 상태에 빠지게 만들기 때문입니다.
*   **원리**: 임계 동작을 락-프리 방식으로 처리하기 위해서는 각 동작을 **트랜잭션(transaction)**으로 생각해야 합니다. 각 트랜잭션은 **전체가 성공하거나 전체가 실패**합니다 (일부만 성공하고 일부만 실패하는 경우는 없음).
*   **실행**: 대부분의 작업을 로컬로 처리한 후, 트랜잭션을 커밋(commit)할 준비가 되면 **하나의 원자적 명령어** (예: CAS, LL/SC)를 실행합니다. 이 원자적 명령어가 성공하면 트랜잭션이 성공적으로 반영된 것이지만, 실패할 경우 다른 스레드가 동시에 트랜잭션을 커밋하고 있음을 의미하므로 다시 시도해야 합니다.
*   **결과**: 이와 같은 실패 후 재시도(fail-and-retry) 방법은 작동할 수밖에 없으며, 그 결과 **최소한 시스템의 한 스레드는 언제나 일을 처리하게 됩니다** (우리 스레드가 아닐지라도). 이것이 바로 락-프리(lock-free)의 정의입니다.

#### 4.9.9 락-프리 연결 리스트 (Lock-Free Linked Lists)

*   **예시**: 락-프리 단일 연결 리스트(singly linked lists)에 `push_front()` 동작을 구현하는 것을 통해 실제 락-프리 트랜잭션의 예를 살펴볼 수 있습니다.
*   **구현 단계**:
    1.  **트랜잭션 준비**: 새로운 `Node`를 할당하고 데이터를 채운 다음, 이 노드의 `m_pNext` 포인터가 리스트의 현재 `m_head`를 가리키게 합니다.
    2.  **원자적 커밋 시도**: `m_head` 포인터에 대해 `compare_exchange_weak()`를 호출합니다. `m_head`는 `Node*`에 대한 원자적 포인터 타입입니다.
    3.  **성공/실패**: 이 호출이 성공하면 새로운 노드가 리스트의 머리에 삽입된 것이고 트랜잭션은 끝납니다. 그러나 실패할 경우 (아마 다른 스레드가 리스트의 머리를 갱신했을 것임) 다시 시도해야 합니다. 이 과정에는 새 노드의 `m_pNext` 포인터를 최신 `m_head` 노드에 대한 주소로 갱신하는 것이 포함됩니다.

#### 4.9.10 락-프리 프로그래밍에 대한 추가 자료들 (Additional Resources on Lock-Free Programming)

*   병행성은 매우 광대하고 심도 깊은 주제이므로, 이 책의 내용은 맛보기에 불과합니다. 더 깊이 학습하고자 하는 독자들은 다음 온라인 자료들을 참조할 수 있습니다:
    *   락-프리 연결 리스트 구현: 허브 서터(Herb Sutter)의 CppCon2014 강연 `https://www.youtube.com/watch?v=clgO9aB9nbs`, `https://www.youtube.com/watch?v=CmxkPChOcvw`.
    *   병행 프로그래밍 전반: 지오프 랭데일(Geoff Langdale)의 CMU 강좌 `https://www.cs.cmu.edu/~410-s05/lectures/L31_LockFree.pdf`.
    *   락-프리 소개: 새미 알 바흐라(Sammy Al-Bahra)의 프레젠테이션 `http://concurrencykit.org/presentations/lockfree_introduction/#/`.
    *   병행적 사고: 마이크 액턴(Mike Acton)의 글 `http://cellperformance.beyond3d.com/articles/public/concurrency_rabit_hole.pdf`.
    *   온라인 도서: 'Little Book of Semaphores' `http://greenteapress.com/semaphores/LittleBookOfSemaphores.pdf`와 'Linux Kernel Memory Barriers' `https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.2011.01.02a_pdf`.
    *   제프 프레싱(Jeff Preshing)의 블로그: 락-프리 프로그래밍, 원자적 동작, 장벽, 펜스 등에 대한 훌륭한 글들 `http://preshing.com/20120612/an-introduction-to-lock-free-programming`.
    *   리눅스의 메모리 장벽: `https://www.mjmwired.net/kernel/Documentation/memory-barriers.txt#305`.

### 4.10 SIMD/벡터 프로세싱 (SIMD/Vector Processing)

*   **개념**: **SIMD (Single Instruction, Multiple Data)**는 현대 마이크로프로세서의 기능으로, **하나의 기계어 명령을 통해 여러 데이터에 대해 산술 연산을 병렬로 수행**하는 것을 뜻합니다.
*   **역사**:
    *   **MMX (Multimedia Extensions)**: 1994년 인텔이 펜티엄 계열 CPU에 처음 도입한 명령어 집합. 8개의 8비트 정수, 4개의 16비트 정수, 2개의 32비트 정수를 64비트 레지스터에 모아 SIMD 연산을 수행했습니다.
    *   **SSE (Streaming SIMD Extensions)**: 펜티엄 III에서 등장한 128비트 레지스터 명령어 집합으로, 정수나 부동소수점 데이터를 담을 수 있습니다. 게임 엔진에서는 주로 32비트 부동소수점 4개를 한 번에 처리하는 데 사용됩니다. SSE2, SSE3, SSSE3, SSE4 등 여러 개정 버전이 있습니다.
    *   **AVX (Advanced Vector Extensions)**: 2011년 인텔이 발표한 더 큰 SIMD 레지스터(256비트) 및 명령어 집합으로, 최대 8개의 32비트 부동소수점 쌍에 대한 연산을 병렬로 처리할 수 있습니다. AVX2는 AVX의 확장이며, 일부 인텔 CPU는 16개의 32비트 부동소수점을 512비트 레지스터에서 처리하는 AVX-512를 지원합니다.

#### 4.10.1 SSE 명령어 집합과 레지스터 (SSE Instruction Set and Registers)

*   **SSE 레지스터**: `XMMi` (i는 0부터 15 사이의 정수)로 표기하며, 128비트 XMMi 레지스터 1개는 4개의 32비트 부동소수점을 저장합니다.
*   **AVX 레지스터**: `YMMi`로 명명하며 256비트 크기입니다.
*   **AVX-512 레지스터**: `ZMMi`로 명명하며 512비트 크기입니다.
*   **원소 지칭**: 각 SSE 레지스터 안의 개별 부동소수점들을 `x, y, z, w` 또는 `r0, r1, r2, r3` 등으로 지칭합니다.

#### 4.10.1.1 `_m128` 데이터 타입

*   **개념**: C/C++ 컴파일러에서 SSE 및 AVX 데이터를 쉽게 취급할 수 있도록 **패킹된(packed) float 배열을 나타낼 특수한 데이터 타입**을 지원합니다.
    *   `__m128` 타입은 4개의 패킹된 float 배열을 표현하며, AVX 내장 함수용으로 `__m256` (8개 float), `__m512` (16개 float) 타입도 있습니다.
*   **활용**: `__m128` 타입은 지역 변수, 함수 인자, 리턴 타입에서 쓰일 수 있으며, 클래스와 구조체의 멤버로도 쓰일 수 있습니다. 지역 변수와 함수 인자는 보통 SSE 레지스터에 대한 직접 접근으로 처리되지만, 전역 변수나 구조체/클래스 멤버인 경우 메모리의 16바이트 정렬된 float 배열로 취급됩니다.
*   **주의**: SSE 연산에서 메모리에 저장된 `__m128` 변수를 사용할 경우, 컴파일러는 데이터를 메모리로부터 SSE 레지스터로 불러오고 결과를 다시 저장하는 명령어를 삽입하므로, 불필요한 메모리 접근이 발생하지 않는지 **디스어셈블리**를 확인하는 것이 좋습니다.

#### 4.10.1.2 SSE 데이터의 메모리 정렬 (Memory Alignment for SSE Data)

*   XMMi 레지스터에 올라갈 데이터는 메모리에 있을 경우 반드시 **16바이트(128비트)로 정렬**되어 있어야 합니다 (AVX는 32바이트, AVX-512는 64바이트).
*   **컴파일러의 자동 정렬**: 전역 변수, 지역 변수, 그리고 구조체/클래스의 `__m128` 멤버에 대해서는 컴파일러가 알아서 메모리 정렬을 맞춰줍니다.
*   **동적 할당의 경우**: 구조체나 클래스를 동적으로 할당한 경우에는 프로그래머가 직접 메모리 정렬을 맞춰야 합니다. `C++11`의 `alignas` 지시어를 사용하거나 정렬된 메모리 할당 방식을 사용합니다.

#### 4.10.1.3 SSE 컴파일러 내장 명령어 (SSE Compiler Intrinsic Instructions)

*   **개념**: SSE 및 AVX 어셈블리 명령어를 직접 사용하는 것은 이식성이 낮고 복잡합니다. 이를 해결하기 위해 컴파일러는 **내장 명령어(intrinsic instructions)**를 지원합니다. 이들은 C 함수와 문법과 동작이 비슷하지만, 컴파일러가 인라인 어셈블리 코드로 바꿔줍니다.
*   **사용법**: SSE 및 AVX 내장 명령을 사용하려면 `<xmmintrin.h>` (Visual Studio) 또는 `<x86intrin.h>` (GCC) 헤더를 포함해야 합니다.

#### 4.10.1.4 유용한 SSE 내장 명령들 (Useful SSE Intrinsics)

*   `__m128 _mm_set_ps(float w, float z, float y, float x)`: 주어진 4개의 부동소수점으로 `__m128` 타입 변수를 초기화합니다 (인자 순서가 `x, y, z, w`와 반대).
*   `__m128 _mm_load_ps(const float* pData)`: C 언어 형식의 `float` 배열을 `__m128` 타입 변수로 로드합니다 (입력 배열은 16바이트 정렬되어 있어야 함).
*   `void _mm_store_ps(float* pData, __m128 v)`: `__m128` 변수를 C 언어 형식의 `float` 배열로 저장합니다 (배열은 16바이트 정렬되어 있어야 함).
*   `__m128 _mm_add_ps(__m128 a, __m128 b)`: `a`와 `b`에 저장된 4개의 `float`를 병렬로 더한 후 결과를 리턴합니다.
*   `__m128 _mm_mul_ps(__m128 a, __m128 b)`: `a`와 `b`에 저장된 4개의 `float`를 병렬로 곱한 후 결과를 리턴합니다.
*   `_mm_set_ps()`의 인자 순서가 역순인 것은 인텔 CPU가 리틀 엔디언(little-endian)이며, SSE 레지스터 내에 성분들이 저장되는 순서가 메모리에 저장될 때의 순서와 반대이기 때문입니다.

#### 4.10.1.5 AltiVec 벡터 타입 (AltiVec Vector Types)

*   **개념**: GNU C/C++ 컴파일러(GCC)는 PowerPC의 AltiVec 명령어 집합을 지원하는데, 이는 인텔 프로세서의 SSE와 마찬가지로 SIMD 연산을 수행합니다.
*   **선언**: 128비트 벡터 타입을 일반적인 C/C++ 타입처럼 선언할 수 있지만, `vector` 키워드가 앞에 붙어야 합니다 (예: `vector float v;`는 4개의 `float` 변수로 이루어진 SIMD 벡터).

#### 4.10.2 SSE를 사용한 루프 벡터화 (Loop Vectorization using SSE)

*   **목표**: SIMD를 사용하면 특정 연산의 처리량을 4배 증가시킬 수 있습니다 (한 기계어 명령을 통해 병렬로 4개의 부동소수점 연산을 처리).
*   **예시 (배열 덧셈)**:
    *   **참조 구현 (`AddArrays_ref`)**: 일반적인 `for` 루프를 사용하여 배열의 각 원소를 순차적으로 더합니다.
    *   **SSE 벡터화 구현 (`AddArrays_sse`)**: `for` 루프를 한 번에 4개씩 순환하도록 변경합니다. 4개의 `float` 값을 SSE 레지스터에 로드하고 병렬로 더한 후, 계산 결과를 결과 배열에 4개씩 순서대로 저장합니다.
*   **효과**: 벡터화는 상당한 성능 향상을 가져옵니다. 위의 코드는 이론적인 4배까지는 아니지만, 매우 큰 `float` 배열에 대해 비교해 보면 벡터화를 적용하지 않은 함수보다 약 **3.8배 더 빠른** 성능을 보였습니다.

#### 4.10.3 내적 벡터화 (Dot Product Vectorization)

*   **목표**: 4원소로 된 부동소수점 벡터 배열 두 개가 있을 때, 각 쌍의 내적을 계산하고 결과를 `float` 배열에 저장하는 것입니다.

#### 4.10.3.1 첫 번째 시도 (느린 구현)

*   **문제점**: `_mm_hadd_ps()`와 같은 **수평(horizontal) 덧셈** 명령어를 사용합니다. 이 명령어는 하나의 레지스터에 대해 두 가지 덧셈을 수행하며, 최종 결과를 얻기 위해 여러 번 사용될 수 있습니다. 하지만 **레지스터를 횡(horizontal)으로 더하는 일은 매우 느린 작업**이라 자주 사용하기에는 부담스럽습니다. 프로파일링 결과 참조 구현보다 오히려 느린 성능을 보이기도 합니다.

#### 4.10.3.2 더 나은 접근법 (Better Approach)

*   **핵심**: 내적 계산에서 SIMD 병렬화의 이점을 최대한 끌어내려면 **레지스터의 횡 덧셈을 피하는 것**입니다.
*   **전치 (Transpose)**: 이를 위해 먼저 입력 벡터의 값들을 **전치(transpose)**해야 합니다. 이렇게 전치된 순서로 벡터를 저장하면 `float` 연산 때와 비슷한 방식으로 내적을 계산할 수 있습니다. 즉, `x` 원소들끼리 곱하고 그 결과를 `y` 원소 곱에 더하고, 다시 `z` 원소 곱에 더하고, 마지막으로 `w` 원소 곱에 그 결과를 더하는 방식입니다.
*   **`madd` (Multiply-Add)**: 곱셈 바로 뒤에 덧셈을 하는 연산은 매우 흔하여 **`madd`**라는 전용 이름이 있습니다. 일부 CPU는 단일 SIMD 명령어를 통해 `madd` 연산을 지원합니다 (예: PowerPC AltiVec의 `vec_madd()`).

#### 4.10.3.3 한 번에 전치하기 (Transposing in One Go)

*   위 예시에서는 함수 호출 부분에서 이미 입력 데이터를 전치했다고 가정했습니다. 하지만 참조 구현과 같은 형식의 입력을 처리하려면 함수 내에서 전치를 수행해야 합니다.
*   **`_MM_TRANSPOSE4_PS` 매크로**: 이 매크로는 4개의 입력 레지스터의 원소들을 섞어서 4개의 레지스터에 저장하는 작업을 합니다. 이 명령어는 그다지 비싸지 않기 때문에 내적 연산과 동시에 수행해도 큰 부담이 되지 않습니다.
*   **성능**: 함수 안에서 연산과 동시에 전치를 하는 버전은 참조 구현보다 대략 **3.5배 빠릅니다**.

#### 4.10.3.4 셔플과 전치 (Shuffle and Transpose)

*   `_MM_TRANSPOSE4_PS` 매크로 내부에서는 `_mm_shuffle_ps()` 내장 명령어를 사용하며, **셔플 마스크(shuffle mask)**라는 4원소 비트 필드를 통해 원소들을 어떻게 옮길 것인지를 지정합니다.

#### 4.10.4 SSE로 벡터-행렬 곱 (SSE Vector-Matrix Multiplication)

*   4원소 벡터와 4x4 행렬을 SSE로 곱하는 것은 입력 벡터와 입력 행렬의 각 4행에 대해 내적 연산을 하는 것과 같습니다.
*   **`Mat44` 클래스**: 행렬의 각 행을 4개의 SSE 벡터로 추상화하며, `union`을 사용하여 행렬의 각 행을 `float` 타입으로도 접근할 수 있게 합니다.
*   **`MulVecMat_sse` 함수**: 입력 벡터를 전치하여 각 성분(x, y, z, w)을 동일하게 복제한 SSE 레지스터를 만들고, 이를 행렬의 행과 내적 계산하는 방식으로 구현됩니다.

#### 4.10.5 SSE로 행렬끼리의 곱 (SSE Matrix-Matrix Multiplication)

*   벡터와 행렬 곱을 구현했으니, 4x4 행렬 두 개를 SSE로 곱하는 일은 `MulVecMat_sse` 함수를 재활용하여 간단히 구현할 수 있습니다.

#### 4.10.6 벡터화의 일반화 (Generalizing Vectorization)

*   SIMD 레지스터의 각 성분들은 그 위에 임의의 동작을 수행할 수 있는 나란한 **'레인(lane)'**과 같습니다.
    *   128비트(4원소) SIMD 변수는 한 번에 4개의 레인에 연산을 수행할 수 있습니다.
    *   256비트 AVX 레지스터는 8개의 레인에, AVX-512는 16개의 레인에 연산이 가능합니다.
*   **코드 작성 방법**: 벡터화하는 코드를 짜는 가장 쉬운 방법은 일단 레인 하나에 해당하는 알고리듬(단일 레인 알고리듬) 하나만 먼저 짜는 것입니다. 일단 이 구현이 동작한다면, 한 번에 N개를 처리하도록 바꾸면 됩니다.
*   **장점**: 이렇게 벡터화를 할 경우, 약간만 고치면 더 큰 SIMD 하드웨어(예: AVX, AVX-512)를 저절로 활용할 수 있는 이점이 있습니다.
*   **컴파일러 자동 벡터화**: 최적화 기능이 있는 대부분의 컴파일러는 특정 단일 레인 루프에 대해 자동으로 벡터화할 수 있습니다.

#### 4.10.7 SIMD 프레디케이션 (SIMD Predication)

*   **문제**: 배열의 각 원소의 제곱근을 구할 때, 음수의 제곱근은 허수(QNaN)가 되므로 입력 값이 0 이상인지 검사해야 합니다. 하지만 SIMD 비교 명령어(예: `_mm_cmpge_ps()`)는 불리언이 아닌, 각 성분별 결과를 4개의 32비트 마스크로 리턴하므로 단순한 `if` 문으로 비교할 수 없습니다.
*   **벡터 프레디케이션 (Vector Predication)**: SIMD 비교 명령어의 결과를 **비트 마스크**로 사용하여 두 가지 가능한 결과 중 하나를 선택하는 방식입니다.
    *   입력 값이 테스트를 통과하면 (0보다 같거나 큰 경우) 그 값의 제곱근을 선택하고, 실패하면 (음수인 경우) 0을 선택합니다.
*   **구현**: 비트와이즈 AND, OR, NOT 연산자를 SSE 버전으로 교체하여 구현합니다.
*   **벡터 선택 (Vector Select)**: 보통 이러한 벡터 프레디케이션을 함수로 감싸는 것이 편리하며, 이를 '벡터 선택'이라고 부릅니다. AltiVec ISA는 `vec_sel()`이라는 내장 명령을 지원하며, SSE4에서는 `_mm_blendv_ps()`를 지원합니다.
*   **수동 구현**: `_mm_select_ps()` 함수를 직접 비트와이즈 연산자(AND, OR, NOT)를 사용하여 구현할 수 있습니다. XOR 연산으로도 구현할 수 있습니다.

### 4.11 GPGPU 프로그래밍에 대한 소개 (Introduction to GPGPU Programming)

*   **GPU (Graphics Processing Unit)**: **데이터 병렬 연산**에 매우 빠르게 처리할 수 있도록 디자인된 코프로세서입니다.
*   **설계 원리**: SIMD 병렬화(벡터화 ALU)와 MIMD 병렬화(선점형 멀티스레딩)를 결합하여 이루어집니다.
*   **SIMT (Single Instruction, Multiple Threads)**: NVIDIA가 이 같은 SIMD/MIMD 복합 디자인을 설명하기 위해 만든 용어입니다. GPU 구현은 제조사나 품목마다 크게 차이 나지만, 모든 GPU는 SIMT 병렬화 개념을 응용하여 디자인됩니다.
*   **데이터 병렬 연산의 적합성**: GPU는 매우 큰 데이터 셋에 대해 데이터 병렬 연산을 전문적으로 처리하도록 디자인되었습니다. 연산 작업이 GPU에서 실행하기 적합한 형태가 되려면 데이터 셋의 각 성분에 행해지는 연산들이 다른 성분의 계산 결과에 독립적이어야 합니다.
*   **예시 (DotArrays_ref())**: 루프에서 한 주기 동안 수행하는 연산은 다른 주기의 연산과는 독립적입니다. 따라서 어떤 순서로 연산을 하건 상관없으며, SIMD 병렬화를 통해 4개, 8개, 16개의 연산을 동시에 처리하여 반복 횟수를 크게 줄일 수 있습니다.
*   **GPU의 확장성**: GPU는 SIMD 유닛을 여러 개 가지며, 이 같은 큰 일감을 이 SIMD 유닛들에 병렬로 넘길 수 있어 말 그대로 **수천 개의 데이터 성분들을 병렬로 처리**할 수 있습니다.
*   **활용 분야**: 픽셀/프래그먼트 셰이더(수백만 개의 픽셀 처리), 버텍스 셰이더(수십만~수백만 개의 3D 메시 처리), 그리고 **계산 셰이더(compute shaders)**를 통해 프로그래머가 활용할 수 있게 합니다. 계산이 서로 독립적이기만 하다면, 큰 데이터 셋에 대한 연산을 처리하는 데 CPU보다 GPU가 더 효율적일 가능성이 큽니다.

#### 4.11.2 컴퓨트 커널 (Compute Kernel)

*   **개념**: GPGPU 컴퓨트 셰이더를 짤 때는 SIMD 벡터화에서처럼 루프를 하드코딩하지 않고, 루프를 **스칼라 데이터 타입의 '단일 레인' 용도로 그대로 둔 다음**, 이 루프를 **커널(kernel)**이라 불리는 **별도의 함수**로 분리합니다.
*   **예시 (`DotKernel()`)**: 이 함수는 입력 데이터의 1개 원소만 처리하고 1개의 결과만을 출력합니다. 이는 픽셀 셰이더가 1개의 픽셀을 입력받아 1개의 출력 색으로 변환하거나, 정점 셰이더가 1개의 정점을 입력받아 1개의 정점을 출력하는 것과 유사합니다. GPU가 데이터 원소 1개마다 커널 함수를 대신 호출해 주는 것이라고 생각할 수 있습니다.
*   **셰이딩 언어**: GPGPU 컴퓨트 커널은 주로 특수한 셰이딩 언어(DirectX의 HLSL, OpenGL의 GLSL, NVIDIA의 CUDA C, OpenCL)로 작성됩니다. 이 언어들은 C 언어 문법과 매우 유사하여 C/C++ 루프를 GPU 컴퓨트 커널로 변경하기가 어렵지 않습니다.
*   **CUDA C 예시**: `__global__ void DotKernel_CUDA(...)`와 같이 정의되며, 루프 인덱스 `i`는 커널 함수 내의 `threadIdx.x` 변수에서 자동으로 제공됩니다.

#### 4.11.3 GPU 코드 실행 (GPU Code Execution)

*   **준비 단계**: CPU와 GPU 양쪽에서 모두 접근 가능한 '관리되는(managed)' 버퍼를 할당(`cudaMallocManaged`)하고, 입력 데이터를 이곳으로 복사하는 등의 준비 코드가 필요합니다.
*   **커널 실행**: CUDA C 문법인 세 개의 화살괄호 표기법(`<<<G, N>>>`)을 사용하여 드라이버에 요청을 전달함으로써 컴퓨트 커널을 GPU에서 실행합니다.
    *   **`N` (두 번째 인자)**: 입력 데이터의 차원(dimensions)을 지정합니다. 이는 GPU가 수행해야 할 루프 반복 횟수에 해당합니다.
    *   **`G` (첫 번째 인자)**: 드라이버에게 몇 개의 **스레드 그룹(thread block)**을 사용하여 컴퓨트 커널을 실행할지 지시합니다. `G` 값을 1보다 크게 하면 드라이버가 여러 개의 컴퓨트 유닛에 작업을 분배하도록 합니다.
*   **동기화**: `cudaDeviceSynchronize()` 함수는 GPU가 작업을 마칠 때까지 CPU가 대기하도록 만듭니다 (다른 스레드의 완료를 기다리는 `pthread_join()`과 유사).

#### 4.11.4 GPU 스레드와 스레드 그룹 (GPU Threads and Thread Groups)

*   **GPU 스레드**: GPU 커널을 컴파일하면 GPU 기계어 명령들의 흐름으로 이루어진 명령어 스트림이 됩니다. 이는 하나 이상의 코어에서 실행 가능한 기계 명령어 스트림을 나타낸다는 점에서 CPU 스레드와 동일합니다. 하지만 GPU는 스레드를 실행하는 방식에서 CPU와 차이가 있으므로, GPU의 '스레드'는 CPU와는 조금 다른 의미를 지닙니다.
*   **GPU 아키텍처**: GPU는 여러 개의 **컴퓨트 유닛(Compute Unit, CU)**으로 이루어져 있으며, 각 CU는 여러 개의 **SIMD 유닛**을 갖습니다.
    *   **CU**: 간소화된 CPU 코어라고 생각할 수 있습니다. 명령어 인출/해석 유닛, 메모리 캐시, 스칼라 ALU, 그리고 벡터 처리를 위한 여러 개의 SIMD 유닛이 들어 있습니다.
    *   **SIMD 유닛**: SIMD를 지원하는 CPU에 있는 벡터 연산 유닛과 거의 똑같은 역할을 합니다.
*   **웨이브프런트 (Wavefront) / 워프 (Warp)**: 드라이버는 입력 데이터 버퍼를 64개의 원소 단위 블록으로 나누며, 이 64개 블록마다 하나의 CU에서 컴퓨트 커널을 호출합니다. 이 같은 실행 단위를 **웨이브프런트(wavefront)** (NVIDIA에서는 **워프(warp)**)라고 합니다.
    *   CU는 커널의 명령어를 하나씩 인출/해석하며, 각 명령어는 SIMD 유닛을 통해 16개의 데이터 원소에 동시에 적용됩니다 (SIMD 유닛이 16레인이라고 가정). SIMD 유닛이 4단계 파이프라인으로 구현되어 완료하는 데 4클럭 주기가 소요될 때, CU는 4주기마다 3주기를 놀리는 대신 같은 명령을 세 번 더 다른 16 데이터 원소에 대해 실행합니다. 이것이 CU의 SIMD가 16레인이지만 웨이브프런트는 64개의 데이터 원소인 이유입니다.
*   **GPU 스레드의 정의**: CU가 명령어 스트림을 실행하는 방식이 독특하기 때문에 GPU '스레드'라는 용어는 **하나의 SIMD 레인**을 지칭하는 것으로 이해할 수 있습니다. 즉, 컴퓨트 커널로 변경하기 전의 벡터화하지 않은 루프의 한 반복 주기를 GPU 스레드라고 생각하면 됩니다.
*   **GPGPU의 이점**: 컴퓨트 커널은 프로그래머가 각 GPU의 벡터화에 대해 신경 쓰지 않아도 되기 때문에 **범용성(portability)**을 가질 수 있습니다.

#### 4.11.4.1 SIMD에서 SIMT로 (From SIMD to SIMT)

*   **SIMT (Single Instruction, Multiple Threads)**는 GPU가 단순한 SIMD(Single Instruction, Multiple Data) 유닛에 그치지 않는다는 점을 강조하기 위해 도입된 용어입니다. 이는 **웨이브프런트 간에 시간 분할**을 구현하기 때문입니다.
*   **시간 분할의 필요성**: SIMD 유닛이 웨이브프런트를 처리할 때, 특정 명령어는 메모리 접근이 필요할 수 있고, 따라서 SIMD 유닛은 메모리 컨트롤러의 응답을 기다리는 동안 오랜 기간 대기해야 합니다. 이와 같은 큰 지연 공간을 채우기 위해 SIMD 유닛은 **여러 개의 웨이브프런트(동일 셰이더 프로그램 또는 무관한 셰이더 프로그램에 속한) 간에 시간 분할을 적용**합니다. 한 웨이브프런트가 지연되면 SIMD 유닛은 다른 웨이브프런트로 **문맥 전환(context switching)**을 해서 계속 작업을 이어갑니다.
*   **효율적인 문맥 전환**: GPU의 SIMD 유닛은 문맥 전환을 매우 빈번하게 수행해야 합니다. CPU에서는 문맥 전환 시 교체되는 스레드의 레지스터 값을 메모리에 보존하지만, GPU에서는 문맥 전환 시마다 각 웨이브프런트의 SIMD 레지스터를 저장하기에는 너무 많은 시간이 걸립니다. 이를 해결하기 위해 각 SIMD 유닛은 **매우 큰 레지스터 파일(register file)을 가집니다**. 이 레지스터 파일의 물리적 레지스터 수는 웨이브프런트가 논리적으로 사용할 수 있는 레지스터의 수보다 몇 배는 많아, 여러 웨이브프런트의 논리 레지스터를 물리적 레지스터에 저장할 수 있습니다. 따라서 **웨이브프런트 간의 문맥 전환에서는 레지스터를 저장하거나 복원하는 일이 필요 없습니다**.

#### 4.11.5 더 읽을거리 (Further Reading)

*   GPGPU 프로그래밍은 방대한 주제이므로, 더 많이 공부하고 싶은 독자들은 다음 온라인 튜토리얼과 자료를 참조할 수 있습니다:
    *   CUDA 프로그래밍 입문: `https://developer.nvidia.com/how-to-cuda-c-cpp`
    *   OpenCL 학습: `https://developer.nvidia.com/opencl`
    *   HLSL 프로그래밍 안내 및 참조 매뉴얼: `https://msdn.microsoft.com/en-us/library/bb509561(v=VS.85).aspx`
    *   OpenGL 셰이딩 언어: `https://www.khronos.org/opengl/wiki/OpenGL_Shading_Language`
    *   AMD RadeonTM GCN 아키텍처 백서: `https://www.amd.com/Documents/GCN_Architecture_whitepaper.pdf`